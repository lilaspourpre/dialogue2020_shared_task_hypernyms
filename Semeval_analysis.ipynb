{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from semeval2016_task13.semeval_taxonomy import *\n",
    "from scoring_program.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_path = \"./semeval2016_task13/data/gs_taxo/EN/food_wordnet_en.taxo\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_path = \"./semeval2016_task13/data/gs_taxo/EN/food_wordnet_en.taxo\" \n",
    "tax = SemEvalTaxonomy(gs_path, use_underscore=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = \"./baselines/predictions/semeval/predicted_TAXI_science_en_only_orphan_hch.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = read_dataset(pred_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NetworkXError",
     "evalue": "The node os is not in the digraph.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\networkx\\classes\\digraph.py\u001b[0m in \u001b[0;36msuccessors\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_succ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'os'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNetworkXError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5dbc809394e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariants\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtrue_hypernym\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_hypernym\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mvariant\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvariants\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\dialogue2020\\dialogue2020_shared_task_hypernyms\\semeval2016_task13\\semeval_taxonomy.py\u001b[0m in \u001b[0;36mget_hypernym\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_underscore\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_hyponym\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\networkx\\classes\\digraph.py\u001b[0m in \u001b[0;36msuccessors\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    794\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_succ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 796\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mNetworkXError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The node %s is not in the digraph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m     \u001b[1;31m# digraph definitions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNetworkXError\u001b[0m: The node os is not in the digraph."
     ]
    }
   ],
   "source": [
    "final_results = {}\n",
    "\n",
    "accuracy = 0\n",
    "\n",
    "for word, variants in predictions.items():\n",
    "    true_hypernym = tax.get_hypernym(word)[0].replace(\"_\", \" \")\n",
    "    \n",
    "    for variant in variants:\n",
    "        if variant in word:\n",
    "            final_results[word] = variant\n",
    "            if variant != true_hypernym:\n",
    "                print(word,\"|\",  true_hypernym, \"|\", variant)\n",
    "                print(variants)\n",
    "                print(true_hypernym == final_results[word])\n",
    "                print(\"====\")\n",
    "    if word not in final_results:\n",
    "        final_results[word] = variants[0]\n",
    "        \n",
    "    if true_hypernym == final_results[word]:\n",
    "        accuracy += 1\n",
    "#     else:\n",
    "#         print(word,\"|\",  true_hypernym)\n",
    "#         if \n",
    "#         print(variants)\n",
    "# #     if (tax.get_hypernym(word)[0].replace(\"_\", \" \") in variants) and (true_hypernym != final_results[word]):\n",
    "# #         print(tax.get_hypernym(word)[0].replace(\"_\", \" \") in variants)\n",
    "# #         print(word,\"|\",  true_hypernym)\n",
    "#     print(\"===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'os': 'history of religion',\n",
       " 'german language and literature': 'linguistics',\n",
       " 'american material culture': 'history',\n",
       " 'paleobiology': 'ecology',\n",
       " 'elementary particles and fields and string theory': 'physics',\n",
       " 'plant biology': 'molecular biology',\n",
       " 'biomechanics': 'physiology',\n",
       " 'spanish literature': 'history',\n",
       " 'cognition and perception': 'mathematics',\n",
       " 'industrial and organizational psychology': 'physics',\n",
       " 'criminology': 'psychology',\n",
       " 'physical processes': 'engineering',\n",
       " 'atomic, molecular and optical physics': 'physics',\n",
       " 'mechanics of materials': 'history of religion',\n",
       " 'astronomy and astrophysics': 'physics',\n",
       " 'bioimaging and biomedical optics': 'science',\n",
       " 'asian art and architecture history': 'history',\n",
       " 'medicine and health': 'public health',\n",
       " 'growth and development': 'mathematics',\n",
       " 'photonics': 'physics',\n",
       " 'american popular culture': 'history',\n",
       " 'heat transfer, combustion': 'engineering',\n",
       " 'aeronautical vehicles': 'engineering',\n",
       " 'process control and systems': 'dynamical systems',\n",
       " 'molecular, cellular, and tissue engineering': 'engineering',\n",
       " 'inequality and stratification': 'mathematics',\n",
       " 'statistical theory': 'mathematics',\n",
       " 'nanotechnology fabrication': 'engineering',\n",
       " 'theory and criticism history': 'science',\n",
       " 'syntax': 'linguistics',\n",
       " 'african languages and societies': 'literature in english, north america',\n",
       " 'paleontology': 'science',\n",
       " 'urban studies': 'ecology',\n",
       " 'epistemology': 'linguistics',\n",
       " 'american studies': 'history',\n",
       " 'the sun and the solar system': 'physics',\n",
       " 'east asian languages and societies': 'history',\n",
       " 'ethnic studies': 'anthropology',\n",
       " 'kinesiology': 'science',\n",
       " 'acoustics, dynamics, and controls': 'physics',\n",
       " 'phonetics': 'linguistics',\n",
       " 'forest biology': 'ecology',\n",
       " 'ancient, medieval, renaissance and baroque art and architecture': 'classics',\n",
       " 'hardware systems': 'engineering',\n",
       " 'space vehicles': 'engineering',\n",
       " 'near eastern languages and societies': 'literature in english, north america',\n",
       " 'international and intercultural communication': 'communication',\n",
       " 'forest management': 'ecology',\n",
       " 'performance studies': 'mathematics',\n",
       " 'social influence and political communication': 'communication',\n",
       " 'behavioral neurobiology': 'psychology',\n",
       " 'work, economy and organizations': 'mathematics',\n",
       " 'french and francophone language and literature': 'physics',\n",
       " 'genomics': 'molecular biology',\n",
       " 'applied dynamics': 'mathematics',\n",
       " 'literature in english, anglophone': 'history of religion',\n",
       " 'pharmaceutics': 'chemistry',\n",
       " 'demography, population, and ecology': 'mathematics',\n",
       " 'public administration': 'communication',\n",
       " 'bacteriology': 'microbiology',\n",
       " 'semantics': 'linguistics',\n",
       " 'external galaxies': 'physics',\n",
       " 'esthetics': 'psychology',\n",
       " 'american art and architecture history': 'history',\n",
       " 'discrete mathematics and combinatorics': 'mathematics',\n",
       " 'politics and social change': 'sociology',\n",
       " 'structural materials': 'engineering',\n",
       " 'parasitology': 'science',\n",
       " 'biomedical': 'science',\n",
       " 'fresh water studies': 'chemistry',\n",
       " 'food biotechnology': 'nutrition',\n",
       " 'seismology': 'science',\n",
       " 'agronomy': 'science',\n",
       " 'stars, interstellar medium and the galaxy': 'physics',\n",
       " 'broadcast studies': 'science',\n",
       " 'catalysis and reaction engineering': 'engineering',\n",
       " 'meteorology': 'science',\n",
       " 'comparative and historical linguistics': 'classics',\n",
       " 'systems and integrative physiology': 'science',\n",
       " 'latin american literature': 'history',\n",
       " 'vulcanology': 'science',\n",
       " 'power and energy': 'physics',\n",
       " 'biological and physical': 'physics',\n",
       " 'biochemical and biomolecular engineering': 'engineering',\n",
       " 'systems and communications': 'communication',\n",
       " 'plasma and beam physics': 'physics',\n",
       " 'glaciology': 'science',\n",
       " 'biometry': 'science',\n",
       " 'south and southeast asian languages and societies': 'literature in english, north america',\n",
       " 'engineering mechanics': 'engineering',\n",
       " 'botany': 'science',\n",
       " 'gender and sexuality': 'social psychology',\n",
       " 'pulp/paper technology': 'science',\n",
       " 'biological and chemical physics': 'physics',\n",
       " 'nature and society relations': 'mathematics',\n",
       " 'systems and integrative engineering': 'engineering',\n",
       " 'slavic languages and societies': 'physics',\n",
       " 'multi-vehicle systems': 'engineering',\n",
       " 'composition': 'chemistry',\n",
       " 'french and francophone literature': 'classics',\n",
       " 'place and environment': 'physics',\n",
       " 'gender, race, sexuality, and ethnicity in communication': 'literature in english, north america',\n",
       " 'language description': 'linguistics',\n",
       " 'structures and materials': 'physics',\n",
       " 'american film studies': 'history',\n",
       " 'aerodynamics': 'engineering',\n",
       " 'byzantine and modern greek': 'history',\n",
       " 'portuguese literature': 'linguistics',\n",
       " 'biblical studies': 'anthropology',\n",
       " 'speech and rhetorical studies': 'physics',\n",
       " 'human computer interfaces': 'science',\n",
       " 'navigation, guidance, control and dynamics': 'mathematics',\n",
       " 'electro-mechanical systems': 'engineering',\n",
       " 'complex fluids': 'physics',\n",
       " 'german literature': 'history',\n",
       " 'electronic devices and semiconductor manufacturing': 'engineering',\n",
       " 'european languages and societies': 'physics',\n",
       " 'virology': 'microbiology',\n",
       " 'aquaculture and fisheries': 'engineering',\n",
       " 'social control, law, crime, and deviance': 'sociology',\n",
       " 'tectonics and structure': 'physics',\n",
       " 'industrial organization': 'engineering',\n",
       " 'literature in english, british isles': 'history',\n",
       " 'polymer and organic materials': 'chemistry',\n",
       " 'semiconductor and optical materials': 'physics',\n",
       " 'models and methods': 'physics',\n",
       " 'public relations/advertising': 'science',\n",
       " 'linguistic diversity': 'linguistics',\n",
       " 'language documentation': 'linguistics',\n",
       " 'latin american languages and societies': 'linguistics',\n",
       " 'personality and social contexts': 'sociology',\n",
       " 'rhetoric and composition': 'physics',\n",
       " 'computer-aided engineering and design': 'engineering',\n",
       " 'construction engineering/management': 'engineering',\n",
       " 'propulsion and power': 'engineering',\n",
       " 'pragmatics': 'linguistics',\n",
       " 'theory, knowledge and science': 'science',\n",
       " 'non-linear dynamic': 'mathematics',\n",
       " 'family, life course, and society': 'physics',\n",
       " 'dramatic literature, criticism and theory': 'mathematics',\n",
       " 'harmonic analysis and representation': 'mathematics',\n",
       " 'feminist, gender and sexuality studies': 'social psychology',\n",
       " 'biotransport': 'physiology',\n",
       " 'indo-european linguistics and philology': 'linguistics',\n",
       " 'music theory': 'music',\n",
       " 'journalism studies': 'science',\n",
       " 'agricultural and resource economics': 'economics',\n",
       " 'modern art and architecture history': 'history',\n",
       " 'applied statistics': 'mathematics',\n",
       " 'critical and cultural studies': 'social psychology',\n",
       " 'immunoprophylaxis': 'nutrition',\n",
       " 'data storage systems': 'engineering',\n",
       " 'first language acquisition': 'linguistics',\n",
       " 'religious thought': 'social science',\n",
       " 'second language acquisition': 'linguistics',\n",
       " 'bioelectrical and neuroengineering': 'engineering',\n",
       " 'econometrics': 'economics',\n",
       " 'ceramic materials': 'engineering'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from semeval2016_task13.semeval_taxonomy import *\n",
    "from scoring_program.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torchtext\n",
    "import torchtext.datasets\n",
    "import torchtext.data\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "import fasttext\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_path = \"./semeval2016_task13/data/gs_taxo/EN/food_wordnet_en.taxo\" \n",
    "tax = SemEvalTaxonomy(gs_path, use_underscore=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./semeval2016_task13/data/gs_taxo/EN/food_wordnet_en.taxo', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = tuple(set(text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = set()\n",
    "all_ = set()\n",
    "counter = 0\n",
    "for line in text.split(\"\\n\"):\n",
    "    if line:\n",
    "        _, word, hypernym = line.split(\"\\t\")\n",
    "        pairs.add((word, hypernym))\n",
    "        all_.add(word)\n",
    "        all_.add(hypernym)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1576\n"
     ]
    }
   ],
   "source": [
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "nodes = list(tax.get_nodes())\n",
    "\n",
    "def get_random(word, stop_word):\n",
    "    random_node = choice(nodes)\n",
    "    while random_node == stop_word or random_node == word:\n",
    "        random_node = choice(nodes)\n",
    "    return random_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_hypernyms = dict()\n",
    "for word, hypernym in pairs:\n",
    "    false_hypernyms[word] = get_random(word, hypernym)\n",
    "    false_hypernyms[hypernym] = get_random(word, hypernym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29, 23, 7, 37, 10, 27, 25, 11, 34, 14, 10] [25, 11, 34, 14, 10] 1\n"
     ]
    }
   ],
   "source": [
    "for word, hypernym in pairs:\n",
    "    print([char2int[i] for i in word], [char2int[i] for i in hypernym], 1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29, 23, 7, 37, 10, 27, 25, 11, 34, 14, 10] [26, 10, 32, 0, 7, 14, 14, 23, 7, 8] 0\n"
     ]
    }
   ],
   "source": [
    "for word, hypernym in false_hypernyms.items():\n",
    "    print([char2int[i] for i in word], [char2int[i] for i in hypernym], 0)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('white sauce', 'sauce'),\n",
       " ('mash', 'feed'),\n",
       " ('cocktail', 'mixed drink'),\n",
       " ('cracked wheat bread', 'bread'),\n",
       " ('french omelet', 'firm omelet'),\n",
       " ('chowder', 'soup'),\n",
       " ('white rice', 'rice'),\n",
       " ('compote', 'dessert'),\n",
       " ('self rising flour', 'mix'),\n",
       " ('corn muffin', 'muffin'),\n",
       " ('truffle', 'candy'),\n",
       " ('coq au vin', 'dish'),\n",
       " ('metheglin', 'mead'),\n",
       " ('pork and veal goulash', 'goulash'),\n",
       " ('brandy sling', 'sling'),\n",
       " ('madeira', 'fortified wine'),\n",
       " ('soul food', 'food'),\n",
       " ('schnitzel', 'dish'),\n",
       " ('coca cola', 'cola'),\n",
       " ('bonbon', 'candy'),\n",
       " ('irish', 'whiskey'),\n",
       " ('tuna fish salad', 'salad'),\n",
       " ('mix', 'concoction'),\n",
       " ('bread dough', 'dough'),\n",
       " ('garlic chive', 'flavorer'),\n",
       " ('chili vinegar', 'vinegar'),\n",
       " ('stuffed tomato', 'dish'),\n",
       " ('knish', 'turnover'),\n",
       " ('water', 'food'),\n",
       " ('high tea', 'dinner'),\n",
       " ('salmi', 'ragout'),\n",
       " ('egg', 'foodstuff'),\n",
       " ('lutefisk', 'dish'),\n",
       " ('home brew', 'alcohol'),\n",
       " ('manhattan clam chowder', 'clam chowder'),\n",
       " ('crab louis', 'salad'),\n",
       " ('malt', 'grain'),\n",
       " ('coquille', 'dish'),\n",
       " ('molasses kiss', 'kiss'),\n",
       " ('sugar water', 'drinking water'),\n",
       " ('birch beer', 'soft drink'),\n",
       " ('vanilla', 'flavorer'),\n",
       " ('spotted dick', 'suet pudding'),\n",
       " ('low sodium diet', 'diet'),\n",
       " ('goulash', 'stew'),\n",
       " ('potpie', 'dish'),\n",
       " ('liquor', 'alcohol'),\n",
       " ('caramel', 'candy'),\n",
       " ('canary seed', 'bird feed'),\n",
       " ('cranberry juice', 'fruit juice'),\n",
       " ('cottage cheese', 'cheese'),\n",
       " ('coffee break', 'bite'),\n",
       " ('seasoned salt', 'flavorer'),\n",
       " ('mashed potato', 'potato'),\n",
       " ('curry', 'dish'),\n",
       " ('creme de fraise', 'liqueur'),\n",
       " ('kabob', 'dish'),\n",
       " ('breadstuff', 'foodstuff'),\n",
       " ('soup du jour', 'soup'),\n",
       " ('comfit', 'sweet'),\n",
       " ('center', 'sweet'),\n",
       " ('nipa', 'alcohol'),\n",
       " ('culture medium', 'food'),\n",
       " ('stover', 'fodder'),\n",
       " ('perry', 'alcohol'),\n",
       " ('chardonnay', 'white wine'),\n",
       " ('potato skin', 'snack food'),\n",
       " ('nut bar', 'candy'),\n",
       " ('fish fry', 'cookout'),\n",
       " ('chinese mustard', 'mustard'),\n",
       " ('corn', 'grain'),\n",
       " ('onion roll', 'bun'),\n",
       " ('garlic salt', 'flavorer'),\n",
       " ('buttermilk biscuit', 'biscuit'),\n",
       " ('mint', 'herb'),\n",
       " ('refried beans', 'dish'),\n",
       " ('frozen yogurt', 'yogurt'),\n",
       " ('powdered milk', 'milk'),\n",
       " ('sloe gin', 'gin'),\n",
       " ('apple dumpling', 'dumpling'),\n",
       " ('wish wash', 'beverage'),\n",
       " ('gruyere', 'swiss cheese'),\n",
       " ('coleslaw', 'salad'),\n",
       " ('burton', 'ale'),\n",
       " ('shrimp newburg', 'seafood newburg'),\n",
       " ('mincemeat', 'concoction'),\n",
       " ('sack', 'white wine'),\n",
       " ('blood meal', 'feed'),\n",
       " ('apricot sauce', 'sauce'),\n",
       " ('pickle relish', 'relish'),\n",
       " ('formula', 'milk'),\n",
       " ('mushy peas', 'side dish'),\n",
       " ('porkholt', 'goulash'),\n",
       " ('anchovy pizza', 'pizza'),\n",
       " ('herb', 'flavorer'),\n",
       " ('sauce louis', 'dressing'),\n",
       " ('lacing', 'liquor'),\n",
       " ('junket', 'dessert'),\n",
       " ('flatbread', 'bread'),\n",
       " ('fish meal', 'feed'),\n",
       " ('chicken soup', 'soup'),\n",
       " ('spice', 'flavorer'),\n",
       " ('deviled egg', 'dish'),\n",
       " ('iced coffee', 'coffee'),\n",
       " ('galantine', 'dish'),\n",
       " ('mixed drink', 'alcohol'),\n",
       " ('icing sugar', 'powdered sugar'),\n",
       " ('croquette', 'dish'),\n",
       " ('miso', 'spread'),\n",
       " ('muscadet', 'white wine'),\n",
       " ('wild rice', 'grain'),\n",
       " ('frozen pudding', 'frozen dessert'),\n",
       " ('tuck', 'comestible'),\n",
       " ('bap', 'bread'),\n",
       " ('pineapple juice', 'fruit juice'),\n",
       " ('chicken provencale', 'dish'),\n",
       " ('roquefort', 'bleu'),\n",
       " ('turkish coffee', 'coffee'),\n",
       " ('bread', 'starches'),\n",
       " ('mess', 'nutriment'),\n",
       " ('vitamin bc', 'b complex vitamin'),\n",
       " ('saint emilion', 'claret'),\n",
       " ('bouillon cube', 'flavorer'),\n",
       " ('anchovy paste', 'flavorer'),\n",
       " ('heavy cream', 'cream'),\n",
       " ('nonpareil', 'sweet'),\n",
       " ('k ration', 'field ration'),\n",
       " ('condiment', 'flavorer'),\n",
       " ('macaroni salad', 'pasta salad'),\n",
       " ('spruce beer', 'brew'),\n",
       " ('antipasto', 'appetizer'),\n",
       " ('chocolate ice cream', 'ice cream'),\n",
       " ('codfish ball', 'fish cake'),\n",
       " ('geneva', 'gin'),\n",
       " ('orange peel', 'candied citrus peel'),\n",
       " ('bannock', 'flatbread'),\n",
       " ('bisque', 'soup'),\n",
       " ('v 8 juice', 'juice'),\n",
       " ('philter', 'potion'),\n",
       " ('sapsago', 'swiss cheese'),\n",
       " ('chyme', 'food'),\n",
       " ('sauerbraten', 'dish'),\n",
       " ('timothy', 'hay'),\n",
       " ('rum', 'liquor'),\n",
       " ('rickey', 'mixed drink'),\n",
       " ('yquem', 'white wine'),\n",
       " ('fish loaf', 'meat loaf'),\n",
       " ('suet pudding', 'pudding'),\n",
       " ('skillet bread', 'quick bread'),\n",
       " ('ice cream cone', 'frozen dessert'),\n",
       " ('limburger', 'cheese'),\n",
       " ('fish mousse', 'mousse'),\n",
       " ('congee', 'gruel'),\n",
       " ('homogenized milk', 'milk'),\n",
       " ('pea flour', 'meal'),\n",
       " ('sarsaparilla', 'soft drink'),\n",
       " ('seder', 'supper'),\n",
       " ('ratafia', 'liqueur'),\n",
       " ('honey crisp', 'candy'),\n",
       " ('ginger', 'spice'),\n",
       " ('alphabet soup', 'soup'),\n",
       " ('succotash', 'dish'),\n",
       " ('cup', 'punch'),\n",
       " ('mustard', 'condiment'),\n",
       " ('wheat flour', 'flour'),\n",
       " ('nacho', 'tortilla chip'),\n",
       " ('roughage', 'foodstuff'),\n",
       " ('buckwheat', 'grain'),\n",
       " ('cassiri', 'brew'),\n",
       " ('swedish rye bread', 'rye bread'),\n",
       " ('caper sauce', 'sauce'),\n",
       " ('ragout', 'stew'),\n",
       " ('beef goulash', 'goulash'),\n",
       " ('soft roll', 'bun'),\n",
       " ('wintergreen oil', 'flavorer'),\n",
       " ('jam', 'conserve'),\n",
       " ('egg roll', 'dish'),\n",
       " ('darjeeling', 'black tea'),\n",
       " ('sauerkraut', 'dish'),\n",
       " ('peppermint oil', 'flavorer'),\n",
       " ('bolognese pasta sauce', 'spaghetti sauce'),\n",
       " ('sweet cicely', 'herb'),\n",
       " ('training table', 'board'),\n",
       " ('easter egg', 'candy'),\n",
       " ('shortcake', 'baking powder biscuit'),\n",
       " ('white russian', 'cocktail'),\n",
       " ('blanc', 'white sauce'),\n",
       " ('cheshire cheese', 'cheese'),\n",
       " ('bloody mary', 'cocktail'),\n",
       " ('lollipop', 'candy'),\n",
       " ('beef fondue', 'fondue'),\n",
       " ('timbale', 'dish'),\n",
       " ('danish', 'sweet roll'),\n",
       " ('tea bag', 'tea'),\n",
       " ('won ton', 'soup'),\n",
       " ('hamburger bun', 'bun'),\n",
       " ('chicken mousse', 'mousse'),\n",
       " ('rice', 'starches'),\n",
       " ('high protein diet', 'diet'),\n",
       " ('macedoine', 'dish'),\n",
       " ('bock', 'lager'),\n",
       " ('mole', 'sauce'),\n",
       " ('hard boiled egg', 'boiled egg'),\n",
       " ('turkey stew', 'fricassee'),\n",
       " ('kaiser roll', 'bun'),\n",
       " ('candied apple', 'sweet'),\n",
       " ('highball', 'mixed drink'),\n",
       " ('grain', 'foodstuff'),\n",
       " ('tarragon', 'herb'),\n",
       " ('alpha tocopheral', 'vitamin e'),\n",
       " ('peanut bar', 'candy'),\n",
       " ('martini', 'cocktail'),\n",
       " ('rijsttaffel', 'dish'),\n",
       " ('bubble gum', 'chewing gum'),\n",
       " ('vitamin b2', 'b complex vitamin'),\n",
       " ('perishable', 'foodstuff'),\n",
       " ('rolled biscuit', 'biscuit'),\n",
       " ('angelica', 'herb'),\n",
       " ('stinger', 'cocktail'),\n",
       " ('western omelet', 'firm omelet'),\n",
       " ('cheddar', 'cheese'),\n",
       " ('pirogi', 'turnover'),\n",
       " ('stew', 'dish'),\n",
       " ('sour bread', 'bread'),\n",
       " ('high vitamin diet', 'diet'),\n",
       " ('amaretto', 'liqueur'),\n",
       " ('olive', 'relish'),\n",
       " ('pasta', 'dish'),\n",
       " ('spearmint oil', 'flavorer'),\n",
       " ('lobster newburg', 'seafood newburg'),\n",
       " ('forcemeat', 'stuffing'),\n",
       " ('frittata', 'dish'),\n",
       " ('fodder', 'feed'),\n",
       " ('retsina', 'wine'),\n",
       " ('ice cream cake', 'frozen dessert'),\n",
       " ('claret', 'red wine'),\n",
       " ('tahini', 'spread'),\n",
       " ('sage', 'herb'),\n",
       " ('feed', 'food'),\n",
       " ('jacket potato', 'baked potato'),\n",
       " ('hydromel', 'beverage'),\n",
       " ('barmbrack', 'bread'),\n",
       " ('feed grain', 'feed'),\n",
       " ('salsa', 'condiment'),\n",
       " ('string cheese', 'cheese'),\n",
       " ('bercy', 'sauce'),\n",
       " ('harvey wallbanger', 'cocktail'),\n",
       " ('onion bread', 'bread'),\n",
       " ('pheasant under glass', 'dish'),\n",
       " ('larder', 'commissariat'),\n",
       " ('nougat bar', 'candy'),\n",
       " ('hardbake', 'sweet'),\n",
       " ('raita', 'side dish'),\n",
       " ('sweet corn', 'corn'),\n",
       " ('vegetarianism', 'diet'),\n",
       " ('lemon juice', 'juice'),\n",
       " ('onion bagel', 'bagel'),\n",
       " ('tomato juice', 'juice'),\n",
       " ('grog', 'rum'),\n",
       " ('pea soup', 'soup'),\n",
       " ('carbohydrate loading', 'diet'),\n",
       " ('concoction', 'foodstuff'),\n",
       " ('salisbury steak', 'dish'),\n",
       " ('charlotte', 'dessert'),\n",
       " ('fast food', 'nutriment'),\n",
       " ('pina colada', 'mixed drink'),\n",
       " ('pousse cafe', 'liqueur'),\n",
       " ('dill pickle', 'pickle'),\n",
       " ('bordeaux', 'wine'),\n",
       " ('lorenzo dressing', 'dressing'),\n",
       " ('clabber', 'dairy product'),\n",
       " ('caraway seed bread', 'bread'),\n",
       " ('oil cake', 'feed'),\n",
       " ('negus', 'mulled wine'),\n",
       " ('terrine', 'dish'),\n",
       " ('yak butter', 'butter'),\n",
       " ('poached egg', 'dish'),\n",
       " ('swizzle', 'mixed drink'),\n",
       " ('supper', 'meal'),\n",
       " ('raisin bread', 'bread'),\n",
       " ('confiture', 'sweet'),\n",
       " ('eggdrop soup', 'soup'),\n",
       " ('low fat diet', 'diet'),\n",
       " ('nutriment', 'food'),\n",
       " ('malted milk', 'ingredient'),\n",
       " ('barbecue sauce', 'sauce'),\n",
       " ('tonic', 'soft drink'),\n",
       " ('whey', 'dairy product'),\n",
       " ('mint sauce', 'condiment'),\n",
       " ('meal', 'foodstuff'),\n",
       " ('chicken taco', 'taco'),\n",
       " ('hotdog', 'sandwich'),\n",
       " ('vitamin a2', 'vitamin a'),\n",
       " ('marshmallow fluff', 'spread'),\n",
       " ('sweetmeat', 'sweet'),\n",
       " ('borage', 'herb'),\n",
       " ('goat cheese', 'cheese'),\n",
       " ('mozzarella', 'cheese'),\n",
       " ('marjoram', 'herb'),\n",
       " ('potage', 'soup'),\n",
       " ('cross bun', 'sweet roll'),\n",
       " ('saccharin', 'sweetening'),\n",
       " ('coloring', 'foodstuff'),\n",
       " ('roulade', 'dish'),\n",
       " ('aioli', 'sauce'),\n",
       " ('chenin blanc', 'white wine'),\n",
       " ('bread and butter pickle', 'sweet pickle'),\n",
       " ('starches', 'foodstuff'),\n",
       " ('oolong', 'tea'),\n",
       " ('menu', 'fare'),\n",
       " ('cheese sauce', 'white sauce'),\n",
       " ('chicken cordon bleu', 'dish'),\n",
       " ('ashcake', 'cornbread'),\n",
       " ('sweet', 'dainty'),\n",
       " ('sugar', 'sweetening'),\n",
       " ('ice cream', 'frozen dessert'),\n",
       " ('branch water', 'water'),\n",
       " ('chinese anise', 'spice'),\n",
       " ('grapefruit peel', 'candied citrus peel'),\n",
       " ('eau de vie', 'brandy'),\n",
       " ('green salad', 'tossed salad'),\n",
       " ('firewater', 'liquor'),\n",
       " ('praline', 'candy'),\n",
       " ('gum ball', 'chewing gum'),\n",
       " ('half and half', 'dairy product'),\n",
       " ('lye hominy', 'hominy'),\n",
       " ('salmagundi', 'salad'),\n",
       " ('molded salad', 'salad'),\n",
       " ('ham and eggs', 'dish'),\n",
       " ('milk', 'nutriment'),\n",
       " ('sidecar', 'cocktail'),\n",
       " ('meat loaf', 'loaf of bread'),\n",
       " ('stuffing', 'concoction'),\n",
       " ('souffle', 'dish'),\n",
       " ('altar wine', 'wine'),\n",
       " (\"planter's punch\", 'cocktail'),\n",
       " ('groats', 'grain'),\n",
       " ('criollo', 'cocoa'),\n",
       " ('ice water', 'drinking water'),\n",
       " ('garlic bread', 'bread'),\n",
       " ('mold', 'dish'),\n",
       " ('ice milk', 'frozen dessert'),\n",
       " ('burgundy', 'wine'),\n",
       " ('pink lady', 'cocktail'),\n",
       " ('aqua vitae', 'liquor'),\n",
       " ('rotgut', 'alcohol'),\n",
       " ('fried rice', 'dish'),\n",
       " ('power breakfast', 'breakfast'),\n",
       " ('bialy', 'onion roll'),\n",
       " ('oxtail soup', 'soup'),\n",
       " ('barbecued spareribs', 'dish'),\n",
       " ('drinking water', 'water'),\n",
       " ('emmenthal', 'swiss cheese'),\n",
       " ('fenugreek', 'flavorer'),\n",
       " ('vodka', 'liquor'),\n",
       " ('mince', 'nutriment'),\n",
       " ('rhine wine', 'white wine'),\n",
       " ('bourbon', 'whiskey'),\n",
       " ('whiskey', 'liquor'),\n",
       " ('picnic', 'meal'),\n",
       " ('claret cup', 'cup'),\n",
       " ('refresher', 'beverage'),\n",
       " ('biotin', 'b complex vitamin'),\n",
       " ('chicken stew', 'fricassee'),\n",
       " ('ghee', 'clarified butter'),\n",
       " ('sake', 'alcohol'),\n",
       " ('onion butter', 'spread'),\n",
       " ('club sandwich', 'sandwich'),\n",
       " ('dip', 'condiment'),\n",
       " ('vichy water', 'mineral water'),\n",
       " ('hollandaise', 'sauce'),\n",
       " ('viand', 'dish'),\n",
       " ('multivitamin', 'vitamin pill'),\n",
       " ('orange toast', 'toast'),\n",
       " ('orange juice', 'fruit juice'),\n",
       " ('grand marnier', 'orange liqueur'),\n",
       " ('tomato concentrate', 'concentrate'),\n",
       " ('rob roy', 'manhattan'),\n",
       " ('slop', 'feed'),\n",
       " ('farina', 'meal'),\n",
       " ('jelly bean', 'candy'),\n",
       " ('papaya juice', 'juice'),\n",
       " ('demerara', 'rum'),\n",
       " ('buttermilk', 'milk'),\n",
       " ('quesadilla', 'burrito'),\n",
       " ('armagnac', 'brandy'),\n",
       " ('spritzer', 'mixed drink'),\n",
       " ('nosh', 'bite'),\n",
       " ('sambuca', 'liqueur'),\n",
       " ('virgin mary', 'bloody mary'),\n",
       " ('licorice', 'candy'),\n",
       " ('light beer', 'lager'),\n",
       " ('marmalade', 'conserve'),\n",
       " ('colostrum', 'milk'),\n",
       " ('turnover', 'dish'),\n",
       " ('comfort food', 'food'),\n",
       " ('consomme', 'soup'),\n",
       " ('lyonnaise sauce', 'sauce'),\n",
       " ('brie', 'cheese'),\n",
       " ('cream soda', 'soft drink'),\n",
       " ('rock candy', 'candy'),\n",
       " ('camembert', 'cheese'),\n",
       " ('life saver', 'candy'),\n",
       " ('peanut butter', 'spread'),\n",
       " ('marc', 'brandy'),\n",
       " ('potato', 'starches'),\n",
       " ('cobbler', 'highball'),\n",
       " ('vouvray', 'white wine'),\n",
       " ('wort', 'malt'),\n",
       " ('irish soda bread', 'quick bread'),\n",
       " ('guacamole', 'dip'),\n",
       " ('yorkshire pudding', 'quick bread'),\n",
       " ('gruel', 'porridge'),\n",
       " ('carrot juice', 'juice'),\n",
       " ('skillet corn bread', 'cornbread'),\n",
       " ('host', 'bread'),\n",
       " ('fish stick', 'dish'),\n",
       " ('ration', 'fare'),\n",
       " ('jewish rye bread', 'rye bread'),\n",
       " ('mustard seed', 'flavorer'),\n",
       " ('frozen food', 'foodstuff'),\n",
       " ('paprika', 'flavorer'),\n",
       " ('slumgullion', 'stew'),\n",
       " ('anchovy dressing', 'dressing'),\n",
       " ('chicken broth', 'broth'),\n",
       " (\"goats' milk\", 'milk'),\n",
       " ('cafe royale', 'coffee'),\n",
       " ('french dressing', 'dressing'),\n",
       " ('bottled water', 'drinking water'),\n",
       " ('b complex vitamin', 'water soluble vitamin'),\n",
       " ('chicken kiev', 'dish'),\n",
       " ('maryland chicken', 'dish'),\n",
       " ('crystallized ginger', 'candied fruit'),\n",
       " ('coriander', 'flavorer'),\n",
       " ('allspice', 'spice'),\n",
       " ('liquid diet', 'diet'),\n",
       " ('turkey stuffing', 'stuffing'),\n",
       " ('soave', 'white wine'),\n",
       " ('sauterne', 'white wine'),\n",
       " ('quark cheese', 'cheese'),\n",
       " ('clambake', 'cookout'),\n",
       " ('gluten bread', 'bread'),\n",
       " ('fish stew', 'stew'),\n",
       " ('cat food', 'petfood'),\n",
       " ('jawbreaker', 'hard candy'),\n",
       " ('shawnee cake', 'johnnycake'),\n",
       " ('tempura', 'dish'),\n",
       " ('gefilte fish', 'dish'),\n",
       " ('banana split', 'split'),\n",
       " ('vitamin b6', 'b complex vitamin'),\n",
       " ('hot toddy', 'mixed drink'),\n",
       " ('sauvignon blanc', 'white wine'),\n",
       " ('matzo', 'bread'),\n",
       " ('coquilles saint jacques', 'dish'),\n",
       " ('gimlet', 'cocktail'),\n",
       " ('commissariat', 'food'),\n",
       " ('cornbread', 'quick bread'),\n",
       " ('pastry', 'dough'),\n",
       " ('scotch kiss', 'kiss'),\n",
       " ('stick cinnamon', 'cinnamon'),\n",
       " ('gluten free diet', 'diet'),\n",
       " ('must', 'grape juice'),\n",
       " ('arroz con pollo', 'chicken and rice'),\n",
       " ('scrambled eggs', 'dish'),\n",
       " ('cookout', 'picnic'),\n",
       " ('glogg', 'punch'),\n",
       " ('espagnole', 'sauce'),\n",
       " ('orange soda', 'soft drink'),\n",
       " ('beverage', 'food'),\n",
       " ('malmsey', 'madeira'),\n",
       " ('bigos', 'stew'),\n",
       " ('vitamin c', 'water soluble vitamin'),\n",
       " ('gumbo', 'soup'),\n",
       " ('split pea soup', 'soup'),\n",
       " ('comfrey', 'herb'),\n",
       " ('powdered mustard', 'mustard'),\n",
       " ('anise', 'flavorer'),\n",
       " ('oatmeal', 'meal'),\n",
       " ('fizz', 'beverage'),\n",
       " ('elixir', 'potion'),\n",
       " ('proof spirit', 'alcohol'),\n",
       " ('celery seed', 'flavorer'),\n",
       " ('cheese spread', 'spread'),\n",
       " ('ginger beer', 'beverage'),\n",
       " ('whipping cream', 'cream'),\n",
       " ('tamale pie', 'dish'),\n",
       " ('chocolate pudding', 'pudding'),\n",
       " ('grape juice', 'fruit juice'),\n",
       " ('blood agar', 'agar'),\n",
       " ('millet', 'grain'),\n",
       " ('meuniere butter', 'butter'),\n",
       " ('whip', 'dessert'),\n",
       " ('brown rice', 'rice'),\n",
       " ('olla podrida', 'stew'),\n",
       " ('canary wine', 'white wine'),\n",
       " ('light cream', 'cream'),\n",
       " ('bordelaise', 'sauce'),\n",
       " ('beaujolais', 'burgundy'),\n",
       " ('scrapple', 'dish'),\n",
       " ('flummery', 'pudding'),\n",
       " ('cracker', 'bread'),\n",
       " ('steak tartare', 'dish'),\n",
       " ('gyro', 'sandwich'),\n",
       " ('cassareep', 'flavorer'),\n",
       " ('corn pudding', 'pudding'),\n",
       " ('egg cream', 'soft drink'),\n",
       " ('egg foo yong', 'omelet'),\n",
       " ('quick bread', 'bread'),\n",
       " ('screwdriver', 'cocktail'),\n",
       " ('wiener roast', 'cookout'),\n",
       " ('pruno', 'hooch'),\n",
       " ('brunswick stew', 'stew'),\n",
       " ('pulque', 'alcohol'),\n",
       " ('candy corn', 'candy'),\n",
       " ('scone', 'quick bread'),\n",
       " ('yogurt', 'dairy product'),\n",
       " ('chowchow', 'relish'),\n",
       " ('cheese', 'dairy product'),\n",
       " ('hooch', 'alcohol'),\n",
       " ('fondant', 'candy'),\n",
       " ('dark bread', 'bread'),\n",
       " ('dairy product', 'foodstuff'),\n",
       " ('hungarian sauce', 'sauce'),\n",
       " ('wine', 'alcohol'),\n",
       " ('bacon lettuce tomato sandwich', 'sandwich'),\n",
       " ('loblolly', 'gruel'),\n",
       " ('kiss', 'candy'),\n",
       " ('slug', 'alcohol'),\n",
       " ('dehydrated food', 'foodstuff'),\n",
       " ('demiglace', 'sauce'),\n",
       " ('shirred egg', 'dish'),\n",
       " ('celery salt', 'flavorer'),\n",
       " ('jug wine', 'wine'),\n",
       " ('new england clam chowder', 'clam chowder'),\n",
       " ('meal', 'nutriment'),\n",
       " ('cheese souffle', 'souffle'),\n",
       " ('horehound', 'candy'),\n",
       " ('banana bread', 'quick bread'),\n",
       " ('honey bun', 'sweet roll'),\n",
       " ('cocoa', 'beverage'),\n",
       " ('bite', 'meal'),\n",
       " ('clary sage', 'sage'),\n",
       " ('peach melba', 'dessert'),\n",
       " ('skilly', 'gruel'),\n",
       " ('espresso', 'coffee'),\n",
       " ('flour', 'foodstuff'),\n",
       " ('fluffy omelet', 'omelet'),\n",
       " ('corn dab', 'cornbread'),\n",
       " ('double creme', 'cream'),\n",
       " ('pate', 'spread'),\n",
       " ('loaf of bread', 'bread'),\n",
       " ('canned meat', 'canned food'),\n",
       " ('california wine', 'wine'),\n",
       " ('ricotta', 'cheese'),\n",
       " ('chop suey', 'dish'),\n",
       " ('ginger', 'flavorer'),\n",
       " ('potluck', 'meal'),\n",
       " ('waldorf salad', 'fruit salad'),\n",
       " (\"calf's foot jelly\", 'gelatin'),\n",
       " ('appetizer', 'course'),\n",
       " ('fanny adams', 'canned meat'),\n",
       " ('vitamin k', 'fat soluble vitamin'),\n",
       " ('balanced diet', 'diet'),\n",
       " ('date nut bread', 'quick bread'),\n",
       " ('tabbouleh', 'salad'),\n",
       " ('straw wine', 'dessert wine'),\n",
       " ('candy bar', 'candy'),\n",
       " ('lithia water', 'mineral water'),\n",
       " ('sparkling wine', 'wine'),\n",
       " ('pepsi', 'cola'),\n",
       " ('water biscuit', 'cracker'),\n",
       " ('pavlova', 'dessert'),\n",
       " ('parsley', 'herb'),\n",
       " ('kummel', 'liqueur'),\n",
       " ('candy cane', 'candy'),\n",
       " ('graham cracker', 'cracker'),\n",
       " ('yolk', 'food'),\n",
       " ('mead', 'brew'),\n",
       " ('poulette', 'sauce'),\n",
       " ('ingesta', 'nutriment'),\n",
       " ('souvlaki', 'kabob'),\n",
       " ('baking powder biscuit', 'biscuit'),\n",
       " ('corn chip', 'snack food'),\n",
       " ('peppermint patty', 'patty'),\n",
       " ('weizenbock', 'weissbier'),\n",
       " ('half and half dressing', 'dressing'),\n",
       " ('calvados', 'brandy'),\n",
       " ('chocolate kiss', 'kiss'),\n",
       " ('clove', 'spice'),\n",
       " ('soy sauce', 'condiment'),\n",
       " ('mascarpone', 'cream cheese'),\n",
       " ('divinity', 'fudge'),\n",
       " ('sandwich plate', 'dish'),\n",
       " ('strawberry daiquiri', 'daiquiri'),\n",
       " ('lunch', 'meal'),\n",
       " ('pinot blanc', 'white wine'),\n",
       " ('hot pot', 'stew'),\n",
       " ('clotted cream', 'cream'),\n",
       " ('savory', 'dainty'),\n",
       " ('may wine', 'punch'),\n",
       " ('sassafras', 'flavorer'),\n",
       " ('dill', 'herb'),\n",
       " ('combination salad', 'tossed salad'),\n",
       " ('basil', 'herb'),\n",
       " ('ready mix', 'mix'),\n",
       " ('corn sugar', 'sugar'),\n",
       " ('liquor', 'broth'),\n",
       " ('tea', 'meal'),\n",
       " ('cinnamon', 'spice'),\n",
       " ('cream cheese', 'cheese'),\n",
       " ('limpa', 'rye bread'),\n",
       " ('choline', 'b complex vitamin'),\n",
       " ('cinnamon toast', 'toast'),\n",
       " ('comestible', 'food'),\n",
       " ('turkish delight', 'candy'),\n",
       " ('veal parmesan', 'dish'),\n",
       " ('claret', 'bordeaux'),\n",
       " ('curacao', 'orange liqueur'),\n",
       " ('apple butter', 'conserve'),\n",
       " ('margarita', 'cocktail'),\n",
       " ('square meal', 'meal'),\n",
       " ('cognac', 'brandy'),\n",
       " ('french bread', 'white bread'),\n",
       " ('corn chowder', 'chowder'),\n",
       " ('demerara', 'brown sugar'),\n",
       " ('barley water', 'broth'),\n",
       " ('cocktail sauce', 'sauce'),\n",
       " ('flip', 'mixed drink'),\n",
       " ('reducing diet', 'diet'),\n",
       " ('creme brulee', 'custard'),\n",
       " ('vermouth', 'wine'),\n",
       " ('tea', 'herb'),\n",
       " ('ice cream sundae', 'frozen dessert'),\n",
       " ('summer savory', 'savory'),\n",
       " ('bulgur', 'wheat'),\n",
       " ('beaujolais', 'red wine'),\n",
       " ('hardtack', 'biscuit'),\n",
       " ('board', 'fare'),\n",
       " ('baguet', 'french bread'),\n",
       " ('lemon peel', 'candied citrus peel'),\n",
       " ('bubble and squeak', 'dish'),\n",
       " ('bay leaf', 'herb'),\n",
       " ('taco', 'dish'),\n",
       " ('samosa', 'turnover'),\n",
       " ('verdicchio', 'white wine'),\n",
       " ('hot sauce', 'sauce'),\n",
       " ('soda water', 'drinking water'),\n",
       " ('inositol', 'b complex vitamin'),\n",
       " ('carrot pudding', 'pudding'),\n",
       " ('chocolate mousse', 'mousse'),\n",
       " ('chicken paprika', 'dish'),\n",
       " ('smorgasbord', 'buffet'),\n",
       " ('drambuie', 'scotch'),\n",
       " ('spread', 'condiment'),\n",
       " ('spaghetti', 'pasta'),\n",
       " ('sesame seed', 'flavorer'),\n",
       " ('cayenne', 'flavorer'),\n",
       " ('black pepper', 'pepper'),\n",
       " ('porridge', 'dish'),\n",
       " ('double cream', 'cream cheese'),\n",
       " ('beer', 'brew'),\n",
       " ('puff batter', 'batter'),\n",
       " ('jambalaya', 'dish'),\n",
       " ('certified milk', 'milk'),\n",
       " ('sweet woodruff', 'herb'),\n",
       " ('nonfat dry milk', 'powdered milk'),\n",
       " ('osso buco', 'dish'),\n",
       " ('weissbier', 'ale'),\n",
       " ('firm omelet', 'omelet'),\n",
       " ('popcorn', 'corn'),\n",
       " ('carbonnade flamande', 'dish'),\n",
       " ('eggnog', 'punch'),\n",
       " ('kedgeree', 'dish'),\n",
       " ('koumiss', 'alcohol'),\n",
       " ('clover leaf roll', 'bun'),\n",
       " ('candy', 'sweet'),\n",
       " ('allergy diet', 'diet'),\n",
       " ('lasagna', 'pasta'),\n",
       " ('sweet roll', 'bun'),\n",
       " ('sweet cider', 'cider'),\n",
       " ('suds', 'beer'),\n",
       " ('mulled cider', 'sweet cider'),\n",
       " ('hard cider', 'alcohol'),\n",
       " ('acidophilus milk', 'milk'),\n",
       " ('light diet', 'diet'),\n",
       " ('fish cake', 'patty'),\n",
       " ('dry vermouth', 'vermouth'),\n",
       " ('molasses taffy', 'taffy'),\n",
       " ('tapenade', 'spread'),\n",
       " ('vitamin p', 'water soluble vitamin'),\n",
       " ('bacon and eggs', 'dish'),\n",
       " ('stuffed mushroom', \"hors d'oeuvre\"),\n",
       " ('cardamom', 'flavorer'),\n",
       " ('hotchpotch', 'stew'),\n",
       " ('hard cider', 'cider'),\n",
       " ('cock a leekie', 'soup'),\n",
       " ('sugar candy', 'candy'),\n",
       " ('jelly', 'conserve'),\n",
       " ('sicilian pizza', 'pizza'),\n",
       " ('petfood', 'feed'),\n",
       " ('herb tea', 'tea'),\n",
       " ('syllabub', 'dessert'),\n",
       " ('vitamin b12', 'b complex vitamin'),\n",
       " ('aperitif', 'alcohol'),\n",
       " ('duff', 'pudding'),\n",
       " ('frozen yogurt', 'frozen dessert'),\n",
       " ('lovage', 'herb'),\n",
       " ('beef broth', 'broth'),\n",
       " ('worcester sauce', 'sauce'),\n",
       " ('double gloucester', 'cheese'),\n",
       " ('mulligan stew', 'stew'),\n",
       " ('grist', 'grain'),\n",
       " ('rye', 'whiskey'),\n",
       " ('carrot stick', 'crudites'),\n",
       " ('peach sauce', 'sauce'),\n",
       " ('taffy', 'candy'),\n",
       " ('carob bar', 'candy'),\n",
       " ('marshmallow', 'candy'),\n",
       " ('phosphate', 'soft drink'),\n",
       " ('newburg sauce', 'sauce'),\n",
       " ('popover', 'muffin'),\n",
       " ('rye bread', 'bread'),\n",
       " ('meringue kiss', 'kiss'),\n",
       " ('western', 'sandwich'),\n",
       " ('souchong', 'black tea'),\n",
       " ('vodka martini', 'martini'),\n",
       " ('red wine', 'wine'),\n",
       " ('cud', 'feed'),\n",
       " ('patty', 'dish'),\n",
       " ('ravigote', 'sauce'),\n",
       " ('vanilla pudding', 'pudding'),\n",
       " ('ambrosia', 'dessert'),\n",
       " ('process cheese', 'cheese'),\n",
       " ('oil meal', 'oil cake'),\n",
       " ('hamburger', 'sandwich'),\n",
       " ('hay', 'fodder'),\n",
       " ('gin and tonic', 'highball'),\n",
       " ('challah', 'bread'),\n",
       " ('clear liquid diet', 'liquid diet'),\n",
       " ('soft drink', 'beverage'),\n",
       " ('ouzo', 'liquor'),\n",
       " ('granulated sugar', 'sugar'),\n",
       " ('treacle', 'syrup'),\n",
       " ('batter', 'concoction'),\n",
       " ('carob', 'foodstuff'),\n",
       " ('tomato sauce', 'spaghetti sauce'),\n",
       " ('cotton candy', 'candy'),\n",
       " ('cheese pizza', 'pizza'),\n",
       " ('bread sauce', 'sauce'),\n",
       " ('tea bread', 'bun'),\n",
       " ('horseradish', 'condiment'),\n",
       " ('syrup', 'sweetening'),\n",
       " ('pita', 'flatbread'),\n",
       " ('corn cake', 'cornbread'),\n",
       " ('scotch egg', 'dish'),\n",
       " ('smitane', 'sauce'),\n",
       " ('bulgur pilaf', 'pilaf'),\n",
       " ('bourguignon', 'sauce'),\n",
       " ('mornay sauce', 'cheese sauce'),\n",
       " ('barley sugar', 'hard candy'),\n",
       " ('choc ice', 'chocolate ice cream'),\n",
       " ('snowball', 'frozen dessert'),\n",
       " ('creme de menthe', 'liqueur'),\n",
       " ('zinfandel', 'red wine'),\n",
       " ('chicken sandwich', 'sandwich'),\n",
       " ('curd', 'dairy product'),\n",
       " ('graham bread', 'dark bread'),\n",
       " ('fudge', 'candy'),\n",
       " ('casserole', 'dish'),\n",
       " ('madrilene', 'consomme'),\n",
       " ('mixer', 'beverage'),\n",
       " ('skim milk', 'milk'),\n",
       " ('bear claw', 'sweet roll'),\n",
       " ('pease pudding', 'pudding'),\n",
       " ('hyssop', 'herb'),\n",
       " ('agar', 'culture medium'),\n",
       " ('schnapps', 'liquor'),\n",
       " ('fondue', 'dish'),\n",
       " ('ham sandwich', 'sandwich'),\n",
       " ('kvass', 'brew'),\n",
       " ('stuffed peppers', 'dish'),\n",
       " ('peach ice cream', 'ice cream'),\n",
       " ('bun', 'bread'),\n",
       " ('pizza', 'dish'),\n",
       " ('white bread', 'bread'),\n",
       " ('french toast', 'dish'),\n",
       " ('johnnycake', 'cornbread'),\n",
       " ('stout', 'ale'),\n",
       " ('cocktail', 'appetizer'),\n",
       " ('swiss cheese', 'cheese'),\n",
       " ('chili', 'dish'),\n",
       " ('chili powder', 'flavorer'),\n",
       " ('nut butter', 'spread'),\n",
       " ('molasses', 'syrup'),\n",
       " ('julienne', 'soup'),\n",
       " ('clam chowder', 'chowder'),\n",
       " ('creme caramel', 'custard'),\n",
       " ('mint', 'candy'),\n",
       " ('milk', 'foodstuff'),\n",
       " ('cream sauce', 'white sauce'),\n",
       " ('zombie', 'highball'),\n",
       " ('nosh up', 'meal'),\n",
       " ('matelote', 'fish stew'),\n",
       " ('salad nicoise', 'salad'),\n",
       " ('tostada', 'dish'),\n",
       " ('stuffed cabbage', 'dish'),\n",
       " ('couscous', 'dish'),\n",
       " ('burgoo', 'cookout'),\n",
       " ('montrachet', 'white wine'),\n",
       " ('celery stick', 'crudites'),\n",
       " ('buffet', 'meal'),\n",
       " ('sour', 'cocktail'),\n",
       " ('barbecue', 'cookout'),\n",
       " ('crab cocktail', 'cocktail'),\n",
       " ('fried egg', 'dish'),\n",
       " ('chablis', 'white wine'),\n",
       " ('snack food', 'dish'),\n",
       " ('sherry', 'fortified wine'),\n",
       " ('vichyssoise', 'soup'),\n",
       " ('paella', 'dish'),\n",
       " ('muenster', 'cheese'),\n",
       " ('brown bread', 'bread'),\n",
       " ('chicken feed', 'mash'),\n",
       " ('veloute', 'sauce'),\n",
       " ('kishke', 'dish'),\n",
       " ('hard sauce', 'sauce'),\n",
       " ('marchand de vin', 'sauce'),\n",
       " ('kahlua', 'coffee liqueur'),\n",
       " ('tapioca', 'foodstuff'),\n",
       " ('gumdrop', 'candy'),\n",
       " ('escalope de veau orloff', 'dish'),\n",
       " ('clove', 'garlic'),\n",
       " ('aspartame', 'sweetening'),\n",
       " ('shrimp cocktail', 'cocktail'),\n",
       " ('fines herbes', 'herb'),\n",
       " ('candied citrus peel', 'candied fruit'),\n",
       " ('grape jelly', 'jelly'),\n",
       " ('pepper', 'flavorer'),\n",
       " ('irish stew', 'stew'),\n",
       " ('flan', 'dessert'),\n",
       " ('ice', 'frozen dessert'),\n",
       " ('marrow', 'dainty'),\n",
       " ('pork and beans', 'dish'),\n",
       " ('butter', 'dairy product'),\n",
       " ('danish blue', 'bleu'),\n",
       " ('varietal', 'wine'),\n",
       " ('mimosa', 'mixed drink'),\n",
       " ('potion', 'beverage'),\n",
       " ('wheat germ', 'nutriment'),\n",
       " ('tea like drink', 'beverage'),\n",
       " ('fish chowder', 'chowder'),\n",
       " ('niacin', 'b complex vitamin'),\n",
       " ('fennel', 'spice'),\n",
       " ('mineral water', 'drinking water'),\n",
       " ('tamale', 'dish'),\n",
       " ('wasabi', 'condiment'),\n",
       " ('adobo', 'dish'),\n",
       " ('mocha', 'flavorer'),\n",
       " ('chinese brown sauce', 'sauce'),\n",
       " ('dietary supplement', 'diet'),\n",
       " ('weizenbier', 'weissbier'),\n",
       " ('bavarian cream', 'custard'),\n",
       " ('sun tea', 'tea'),\n",
       " ('caraway', 'herb'),\n",
       " ('beef wellington', 'dish'),\n",
       " ('chocolate fudge', 'fudge'),\n",
       " ('chow', 'fare'),\n",
       " ('oat', 'grain'),\n",
       " ('paddy', 'rice'),\n",
       " ('banquet', 'meal'),\n",
       " ('punch', 'mixed drink'),\n",
       " ('cafe au lait', 'coffee'),\n",
       " ('orange marmalade', 'marmalade'),\n",
       " ('flavorer', 'ingredient'),\n",
       " ('poi', 'dish'),\n",
       " ('lemon drop', 'hard candy'),\n",
       " ('miraculous food', 'food'),\n",
       " ('special', 'dish'),\n",
       " ('mustard sauce', 'sauce'),\n",
       " ('peanut brittle', 'brittle'),\n",
       " ('curd', 'foodstuff'),\n",
       " ('beet sugar', 'sugar'),\n",
       " ('patty', 'candy'),\n",
       " ('gin', 'liquor'),\n",
       " ('meatball', 'dish'),\n",
       " ('cider', 'beverage'),\n",
       " ('pepper sauce', 'sauce'),\n",
       " ('brownie mix', 'ready mix'),\n",
       " ('elixir of life', 'elixir'),\n",
       " ('bouillon', 'broth'),\n",
       " ('salad', 'dish'),\n",
       " ('veal cordon bleu', 'dish'),\n",
       " (\"hunter's sauce\", 'sauce'),\n",
       " ('lemon oil', 'flavorer'),\n",
       " ('purloo', 'stew'),\n",
       " ('nan', 'bread'),\n",
       " ('kosher', 'nutriment'),\n",
       " ('brunch', 'meal'),\n",
       " ('irish coffee', 'coffee'),\n",
       " ('herring salad', 'salad'),\n",
       " ('matzo meal', 'meal'),\n",
       " ('semolina', 'flour'),\n",
       " ('mocha', 'coffee'),\n",
       " ('milk punch', 'punch'),\n",
       " ('smoothie', 'beverage'),\n",
       " ('charlotte russe', 'charlotte'),\n",
       " ('lekvar', 'filling'),\n",
       " ('vitamin a', 'fat soluble vitamin'),\n",
       " ('collins', 'highball'),\n",
       " ('dolmas', 'dish'),\n",
       " ('pearl barley', 'barley'),\n",
       " ('bleu', 'cheese'),\n",
       " ('sourball', 'hard candy'),\n",
       " ('bagel', 'bun'),\n",
       " ('chartreuse', 'liqueur'),\n",
       " ('vitamin pill', 'dietary supplement'),\n",
       " ('lemon balm', 'herb'),\n",
       " ('pepper pot', 'soup'),\n",
       " ('continental breakfast', 'breakfast'),\n",
       " ('stock cube', 'broth'),\n",
       " ('brandy', 'liquor'),\n",
       " ('pantothenic acid', 'b complex vitamin'),\n",
       " ('oatcake', 'quick bread'),\n",
       " ('tabasco', 'hot sauce'),\n",
       " ('sandwich', 'snack food'),\n",
       " ('juice', 'foodstuff'),\n",
       " ('rollmops', \"hors d'oeuvre\"),\n",
       " ('mushroom sauce', 'sauce'),\n",
       " ('spring water', 'water'),\n",
       " ('pinot noir', 'red wine'),\n",
       " ('bavarian blue', 'bleu'),\n",
       " ('guinness', 'stout'),\n",
       " ('business lunch', 'lunch'),\n",
       " ('muscat', 'fortified wine'),\n",
       " ('aquavit', 'liquor'),\n",
       " ('oyster stuffing', 'stuffing'),\n",
       " ('beef burrito', 'burrito'),\n",
       " ('rice', 'grain'),\n",
       " ('dainty', 'nutriment'),\n",
       " ('cheeseburger', 'hamburger'),\n",
       " ('anchovy sauce', 'sauce'),\n",
       " ('russian dressing', 'dressing'),\n",
       " ('chocolate sauce', 'sauce'),\n",
       " ('crabapple jelly', 'apple jelly'),\n",
       " ('sling', 'highball'),\n",
       " ('crescent roll', 'bun'),\n",
       " ('stick', 'butter'),\n",
       " ('scotch woodcock', 'dish'),\n",
       " ('broth', 'soup'),\n",
       " ('moussaka', 'dish'),\n",
       " ('corn whiskey', 'whiskey'),\n",
       " ('table wine', 'wine'),\n",
       " ('cough drop', 'lozenge'),\n",
       " ('bran muffin', 'muffin'),\n",
       " ('orange pekoe', 'black tea'),\n",
       " ('grasshopper', 'cocktail'),\n",
       " ('finger food', 'nutriment'),\n",
       " ('breadstick', 'bread'),\n",
       " ('drip coffee', 'coffee'),\n",
       " ('caesar salad', 'tossed salad'),\n",
       " ('chicken tetrazzini', 'tetrazzini'),\n",
       " ('gelatin', 'dainty'),\n",
       " ('manhattan', 'cocktail'),\n",
       " ('pesto', 'sauce'),\n",
       " ('cake mix', 'ready mix'),\n",
       " ('bitok', 'dish'),\n",
       " ('roux', 'concoction'),\n",
       " ('liebfraumilch', 'rhine wine'),\n",
       " ('bouillabaisse', 'fish stew'),\n",
       " ('french fries', 'potato'),\n",
       " ('scotch', 'whiskey'),\n",
       " ('grated cheese', 'cheese'),\n",
       " ('nutmeg', 'spice'),\n",
       " ('vitamin', 'nutriment'),\n",
       " ('english muffin', 'bread'),\n",
       " (\"cows' milk\", 'milk'),\n",
       " ('baked potato', 'potato'),\n",
       " ('custard', 'dish'),\n",
       " ('scampi', 'dish'),\n",
       " ('bishop', 'mulled wine'),\n",
       " ('tiramisu', 'dessert'),\n",
       " ('sashimi', 'dish'),\n",
       " ('bomber', 'sandwich'),\n",
       " ('medoc', 'bordeaux'),\n",
       " ('thousand island dressing', 'dressing'),\n",
       " ('macrobiotic diet', 'vegetarianism'),\n",
       " ('pimento butter', 'spread'),\n",
       " ('cola extract', 'flavorer'),\n",
       " ('parmesan', 'cheese'),\n",
       " ('piece de resistance', 'dish'),\n",
       " ('omelet', 'dish'),\n",
       " ('fruit salad', 'salad'),\n",
       " ('steak sauce', 'condiment'),\n",
       " ('wassail', 'punch'),\n",
       " ('brittle', 'candy'),\n",
       " ('almond extract', 'flavorer'),\n",
       " ('allemande', 'sauce'),\n",
       " ('fennel seed', 'flavorer'),\n",
       " ('horseradish sauce', 'sauce'),\n",
       " ('brew', 'alcohol'),\n",
       " ...}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ftmodel = fasttext.load_model(\"baselines/models/cc.ru.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_vectors(multi_word):\n",
    "    words = multi_word.replace(\"_\", \" \").split()\n",
    "    return np.sum(get_vectors(words), axis=0)/len(words)\n",
    "\n",
    "def get_vectors(words):\n",
    "    vectors = np.zeros((len(words), 300))\n",
    "    for i, word in enumerate(words):\n",
    "            vectors[i, :] = ftmodel[word]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, pairs, false_hypernyms, fmodel):\n",
    "        self.fasttext = fmodel\n",
    "        self.dataset = list(pairs) + list(false_hypernyms.items())\n",
    "        self.labels = [1]*len(pairs) + [0]*len(false_hypernyms)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        word, hypernym = self.dataset[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        x = torch.LongTensor([char2int[i] for i in word])\n",
    "        y = torch.LongTensor([char2int[i] for i in hypernym])\n",
    "        \n",
    "        seqlen_x, seqlen_y = torch.LongTensor([len(x)]), torch.LongTensor([len(y)])\n",
    "        \n",
    "        fasttext_x = torch.FloatTensor(get_data_vectors(word))\n",
    "        fasttext_y = torch.FloatTensor(get_data_vectors(hypernym))\n",
    "        \n",
    "        return x, fasttext_x, seqlen_x, y, fasttext_y, seqlen_y, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "def pad_collate(batch):\n",
    "    (x, fasttext_x, seqlen_x, y, fasttext_y, seqlen_y, labels) = zip(*batch)\n",
    "    \n",
    "    x = pad_sequence(x, batch_first=True, padding_value=0)\n",
    "    y = pad_sequence(y, batch_first=True, padding_value=0)\n",
    "    \n",
    "    fasttext_x = torch.stack(fasttext_x, 0)\n",
    "    fasttext_y = torch.stack(fasttext_y, 0)\n",
    "\n",
    "    seqlen_x = torch.stack(seqlen_x, 0).squeeze()\n",
    "    seqlen_y = torch.stack(seqlen_y, 0).squeeze()\n",
    "\n",
    "    labels = torch.FloatTensor(labels)\n",
    "    \n",
    "    return x, fasttext_x, seqlen_x, y, fasttext_y, seqlen_y, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = DataLoader(CustomDataset(pairs, false_hypernyms, ftmodel), batch_size=32, shuffle=True, num_workers=0, collate_fn=pad_collate)\n",
    "val_iterator = DataLoader(CustomDataset(pairs, false_hypernyms, ftmodel), batch_size=32, shuffle=True, num_workers=0, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLSTM(torch.nn.Module):\n",
    "    def __init__(self, n_tokens, input_size, num_hidden, output_size, fasttext_size):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.embeddings = nn.Embedding(n_tokens, input_size)\n",
    "        self.lstm = torch.nn.LSTM(input_size, num_hidden, batch_first=True, bidirectional=True)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "        self.dense1 = torch.nn.Linear(fasttext_size*2+4*num_hidden, output_size)\n",
    "        self.dense2 = torch.nn.Linear(output_size, 1)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.batchnorm = nn.BatchNorm1d(output_size)\n",
    "        \n",
    "  \n",
    "    def forward(self, x, fasttext_x, seqlen_x, y, fasttext_y, seqlen_y):\n",
    "        x = self.embeddings(x)\n",
    "        y = self.embeddings(y)\n",
    "        \n",
    "        packed_x = pack_padded_sequence(x, seqlen_x, batch_first=True, enforce_sorted=False)\n",
    "        _, (x, _) = self.lstm(packed_x)\n",
    "        x = x.permute(1,0,2).reshape(x.shape[1], x.shape[0]*x.shape[2])\n",
    "        \n",
    "        packed_y = pack_padded_sequence(y, seqlen_y, batch_first=True, enforce_sorted=False)\n",
    "        _,(y, _) = self.lstm(packed_y)\n",
    "        y = y.permute(1,0,2).reshape(y.shape[1], y.shape[0]*y.shape[2])\n",
    "        \n",
    "        concatenated = torch.cat((x, fasttext_x, y, fasttext_y), -1)\n",
    "        x = self.relu(self.dense1(concatenated))\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.dense2(x))\n",
    "        return x.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharLSTM(50, 256, 512, 128, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.LongTensor([[13, 15, 24, 23, 9, 32, 24, 7, 26, 5, 29, 30, 9, 13, 24, 15], [13, 15, 24, 23, 9, 32, 24, 7, 26, 5, 29, 30, 9, 13, 24, 15], [13, 15, 24, 23, 9, 32, 24, 7, 26, 5, 29, 30, 9, 13, 24, 15]])\n",
    "y = torch.LongTensor([[30, 7, 15, 9, 1, 7, 9, 15, 24, 32, 29, 35, 24, 7, 26, 15, 3, 24, 32, 30],[30, 9, 13, 24, 15]+ [0]*15, [30, 9, 13, 24, 15]+ [0]*15])\n",
    "fasttext_x = torch.FloatTensor(torch.rand((3, 300)))\n",
    "fasttext_y = torch.FloatTensor(torch.rand((3, 300)))\n",
    "seqlen_x = torch.LongTensor([x.shape[-1], x.shape[-1], x.shape[-1]])\n",
    "seqlen_y = torch.LongTensor([20, 5, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7782, 0.0000, 0.0000], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x, fasttext_x, seqlen_x, y, fasttext_y, seqlen_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, iterator, optimizer, criterion, phase='train', epoch=0, writer=None):\n",
    "    is_train = (phase == 'train')\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        for i, batch in enumerate(iterator):            \n",
    "            global_i = len(iterator) * epoch + i\n",
    "            \n",
    "            # unpack batch\n",
    "            x, fasttext_x, seqlen_x, y, fasttext_y, seqlen_y, classes = batch\n",
    "            x, fasttext_x, seqlen_x, y, fasttext_y, seqlen_y = x.cuda(), fasttext_x.cuda(), seqlen_x.cuda(), y.cuda(), fasttext_y.cuda(), seqlen_y.cuda()\n",
    "\n",
    "            # make prediction\n",
    "            predictions = model(x, fasttext_x, seqlen_x, y, fasttext_y, seqlen_y)\n",
    "            answer = torch.round(torch.sigmoid(predictions)).cpu()\n",
    "            \n",
    "            \n",
    "            # calculate loss\n",
    "            loss = criterion(predictions, classes.cuda())\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if is_train:\n",
    "                # make optimization step\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # dump train metrics to tensorboard\n",
    "                if writer is not None and i % 100 == 0 and phase=='train':\n",
    "                    writer.add_scalar(f\"loss/{phase}\", loss.item(), global_i)\n",
    "                if i % 50 == 0:\n",
    "                    print(torch.sum(answer == classes, dim=-1)/float(answer.shape[0]))\n",
    "#                     print(predictions)\n",
    "    \n",
    "    # dump epoch metrics to tensorboard\n",
    "    if writer is not None:\n",
    "        writer.add_scalar(f\"loss_epoch/{phase}\", epoch_loss / len(iterator), epoch)\n",
    "                  \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def setup_experiment(title, logdir=\"./tb\"):\n",
    "    experiment_name = \"{}@{}\".format(title, datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\"))\n",
    "    writer = SummaryWriter(log_dir=os.path.join(logdir, experiment_name))\n",
    "    best_model_path = f\"{title}.baseline.pth\"\n",
    "    return writer, experiment_name, best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name: CharLSTM@23-06-2020-13-55-34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                  | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4688)\n",
      "tensor(0.8125)\n",
      "tensor(0.8750)\n",
      "tensor(0.7500)\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.629\n",
      "\tVal Loss: 0.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|                                                                         | 1/100 [00:03<06:11,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7500)\n",
      "tensor(0.6562)\n",
      "tensor(0.7188)\n",
      "tensor(0.8438)\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.569\n",
      "\tVal Loss: 0.535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|                                                                        | 2/100 [00:07<06:00,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6562)\n",
      "tensor(0.8750)\n",
      "tensor(0.8125)\n",
      "tensor(0.7812)\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.548\n",
      "\tVal Loss: 0.506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|                                                                       | 3/100 [00:10<05:52,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7812)\n",
      "tensor(0.7500)\n",
      "tensor(0.9062)\n",
      "tensor(0.9688)\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.530\n",
      "\tVal Loss: 0.489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|                                                                       | 4/100 [00:14<05:46,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8125)\n",
      "tensor(0.7500)\n",
      "tensor(0.8125)\n",
      "tensor(0.9688)\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.516\n",
      "\tVal Loss: 0.487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|                                                                      | 5/100 [00:17<05:42,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8438)\n",
      "tensor(0.8125)\n",
      "tensor(0.8750)\n",
      "tensor(0.7500)\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.500\n",
      "\tVal Loss: 0.478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|                                                                     | 6/100 [00:21<05:35,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8750)\n",
      "tensor(0.8438)\n",
      "tensor(0.9062)\n",
      "tensor(0.9062)\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.492\n",
      "\tVal Loss: 0.470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  7%|                                                                    | 7/100 [00:25<05:32,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9375)\n",
      "tensor(0.7188)\n",
      "tensor(0.8750)\n",
      "tensor(0.8125)\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.484\n",
      "\tVal Loss: 0.452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|                                                                    | 8/100 [00:28<05:36,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8750)\n",
      "tensor(0.9062)\n",
      "tensor(0.9062)\n",
      "tensor(0.9062)\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.473\n",
      "\tVal Loss: 0.446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|                                                                   | 9/100 [00:32<05:31,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9062)\n",
      "tensor(0.8438)\n",
      "tensor(0.9062)\n",
      "tensor(0.8125)\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.468\n",
      "\tVal Loss: 0.441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|                                                                 | 10/100 [00:36<05:30,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8125)\n",
      "tensor(0.8750)\n",
      "tensor(0.9375)\n",
      "tensor(0.8438)\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.461\n",
      "\tVal Loss: 0.436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 11%|                                                                 | 11/100 [00:39<05:26,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8125)\n",
      "tensor(0.8438)\n",
      "tensor(0.7812)\n",
      "tensor(0.9062)\n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.456\n",
      "\tVal Loss: 0.432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|                                                                | 12/100 [00:43<05:25,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9375)\n",
      "tensor(0.9688)\n",
      "tensor(0.9375)\n",
      "tensor(0.9062)\n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.447\n",
      "\tVal Loss: 0.429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|                                                               | 13/100 [00:47<05:24,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8125)\n",
      "tensor(0.7812)\n",
      "tensor(0.8750)\n",
      "tensor(0.8438)\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.443\n",
      "\tVal Loss: 0.418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 14%|                                                              | 14/100 [00:51<05:17,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8750)\n",
      "tensor(0.9062)\n",
      "tensor(0.9375)\n",
      "tensor(1.)\n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.436\n",
      "\tVal Loss: 0.412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|                                                              | 15/100 [00:54<05:13,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8438)\n",
      "tensor(0.7188)\n",
      "tensor(0.9688)\n",
      "tensor(0.9688)\n",
      "Epoch: 16\n",
      "\tTrain Loss: 0.434\n",
      "\tVal Loss: 0.409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 16%|                                                             | 16/100 [00:58<05:10,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9062)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-159-de723cb9df21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-156-ddfc98e41260>\u001b[0m in \u001b[0;36mrun_epoch\u001b[1;34m(model, iterator, optimizer, criterion, phase, epoch, writer)\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if DO_TRAIN:\n",
    "    model = CharLSTM(len(char2int), 256, 256, 128, 300).cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    writer, experiment_name, best_model_path = setup_experiment(model.__class__.__name__, logdir=\"./charlstm\")\n",
    "    print(f\"Experiment name: {experiment_name}\")\n",
    "\n",
    "    n_epochs = 100\n",
    "    if not DO_TRAIN:\n",
    "        n_epochs = 0\n",
    "\n",
    "    best_val_loss = float('+inf')\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs)):    \n",
    "        train_loss = run_epoch(model, train_iterator, optimizer, criterion, phase='train', epoch=epoch, writer=writer)\n",
    "        val_loss = run_epoch(model, val_iterator, None, criterion, phase='val', epoch=epoch, writer=writer)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02}')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "        print(f'\\tVal Loss: {val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharLSTM(\n",
       "  (embeddings): Embedding(38, 256)\n",
       "  (lstm): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (dense1): Linear(in_features=1624, out_features=128, bias=True)\n",
       "  (dense2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"ganache\"\n",
    "hypernym_candidate = \"meat\"\n",
    "x, y  = [char2int[i] for i in word], [char2int[i] for i in hypernym_candidate]\n",
    "seqlen_x, seqlen_y = len(x), len(y)\n",
    "fasttext_x = torch.FloatTensor([get_data_vectors(word)])#.unsqueeze(0)\n",
    "fasttext_y = torch.FloatTensor([get_data_vectors(hypernym_candidate)])#.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 7]),\n",
       " torch.Size([1, 4]),\n",
       " torch.Size([1]),\n",
       " torch.Size([1]),\n",
       " torch.Size([1, 300]),\n",
       " torch.Size([1, 300]))"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.LongTensor([x])\n",
    "y = torch.LongTensor([y])\n",
    "seqlen_x, seqlen_y = torch.LongTensor([seqlen_x]),  torch.LongTensor([seqlen_y])\n",
    "\n",
    "x.shape, y.shape, seqlen_x.shape, seqlen_y.shape, fasttext_x.shape, fasttext_y.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0', grad_fn=<RoundBackward>)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(torch.sigmoid(model(x.cuda(), fasttext_x.cuda(), seqlen_x.cuda(), y.cuda(), fasttext_y.cuda(), seqlen_y.cuda())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000,  7.8896,  0.5138,  3.1322,  0.0000,  5.3210, 12.8890,  3.7901,\n",
      "         6.0119,  0.0000,  0.0000,  0.0000,  0.0000,  6.7213,  5.0626,  0.0000,\n",
      "         0.0000,  4.3142,  0.0000,  0.0000,  0.0000,  0.0000,  4.1570,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  7.0024,  6.0456,  0.0000,  0.0000,  2.0749],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1.])\n",
      "0.4117552638053894\n",
      "tensor([ 0.0000,  1.7270,  0.0000,  0.0000,  9.7684,  0.0000,  7.7403,  0.0000,\n",
      "         0.0000, 11.1195,  5.4869,  4.3389,  0.0000,  9.9367,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  3.9408,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         4.6579,  0.0000,  0.0000,  0.0000,  0.0000,  5.0753,  7.8502,  6.4034],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.])\n",
      "0.44003820419311523\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 3.9446, 4.1129, 0.0000, 0.0000, 0.0000,\n",
      "        5.1353, 2.6638, 0.0000, 0.0000, 6.2954, 4.5286, 9.7669, 2.0683, 0.0000,\n",
      "        4.8421, 4.3758, 5.7015, 0.0000, 1.6164, 2.3861, 0.0000, 0.0000, 7.5148,\n",
      "        6.9286, 3.8056, 0.0000, 0.0000, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.])\n",
      "0.3639698624610901\n",
      "tensor([5.8081, 6.1233, 0.0000, 4.1428, 3.8793, 0.0000, 1.8724, 4.3800, 7.9636,\n",
      "        4.5408, 0.0000, 0.0000, 3.7359, 0.0000, 1.2239, 0.0000, 4.8675, 0.0000,\n",
      "        0.0000, 0.3032, 1.4793, 0.0000, 0.0000, 5.6587, 0.0000, 0.0000, 0.0000,\n",
      "        4.2437, 6.1875, 4.4659, 1.1249, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0.])\n",
      "0.3522351384162903\n",
      "tensor([ 0.0000,  5.0278,  0.0000,  0.0000,  1.9244,  7.9330,  0.0000,  0.0000,\n",
      "         0.0000,  3.9846,  0.0000,  0.0000,  0.0000,  0.0000,  1.1294,  0.0000,\n",
      "         0.0000,  7.6012, 10.4970,  5.0646,  0.0000,  3.9497,  3.7845,  0.0000,\n",
      "         0.0000,  0.0000,  3.4964,  5.4339,  0.0000,  9.4842,  0.0000,  7.7296],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1.])\n",
      "0.40629342198371887\n",
      "tensor([ 0.0000,  8.8827,  0.8675,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         5.9984,  0.0000,  0.0000,  8.7578,  0.0000,  0.0000, 12.9724,  5.1964,\n",
      "        10.3121,  4.8286,  0.0000,  0.0000,  8.5323,  0.0000,  9.4672,  0.0000],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0.])\n",
      "0.5151243209838867\n",
      "tensor([ 3.9145,  5.1777,  0.0407,  7.5957,  0.0000,  5.4630,  0.0000,  0.0000,\n",
      "         0.0000, 12.8866,  8.2833,  7.4308,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  5.6061,  0.0000,  5.6568,  6.3535,  0.0000,  0.0000,\n",
      "         3.7582,  0.0000,  0.2275,  0.0000,  0.0000,  1.4731,  0.0000,  0.0000],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1.])\n",
      "0.4389212727546692\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 2.2531, 0.0000, 4.6193, 0.0000, 0.0000,\n",
      "        5.9385, 7.9182, 0.0000, 9.8399, 0.0000, 7.5298, 5.6166, 0.0000, 0.6919,\n",
      "        5.8629, 0.0000, 4.4199, 0.0000, 0.0000, 0.0000, 0.0000, 4.1826, 0.0000,\n",
      "        3.3081, 5.0434, 0.0000, 4.9783, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
      "        1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0.])\n",
      "0.408707857131958\n",
      "tensor([ 3.2947,  0.0000,  4.1176,  0.0000,  0.0000,  0.0000,  3.1074,  0.0000,\n",
      "         0.0000,  0.0000,  5.2025,  8.7232,  0.0000,  0.0000,  3.3474,  0.0000,\n",
      "         6.7313,  0.0000, 10.5632,  0.0000,  0.0000,  4.3095,  6.0496,  0.0000,\n",
      "         5.5420,  0.0000,  3.1867,  0.0000,  5.6817,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.])\n",
      "0.4178450405597687\n",
      "tensor([5.0914, 4.6650, 3.8941, 6.2693, 0.0000, 0.0000, 0.0000, 4.8137, 0.0000,\n",
      "        3.2903, 0.0000, 7.2024, 0.0000, 0.0000, 2.8596, 0.0000, 3.2181, 7.4717,\n",
      "        5.6438, 0.0000, 0.0000, 5.4801, 0.0000, 4.0950, 0.0000, 0.0096, 0.6691,\n",
      "        3.9362, 0.0000, 3.4138, 4.2311, 3.1995], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "        1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.])\n",
      "0.3256767690181732\n",
      "tensor([2.5536e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.6158e-03,\n",
      "        0.0000e+00, 0.0000e+00, 4.1599e+00, 5.2926e+00, 5.5531e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 8.7420e+00, 6.0135e+00, 4.9739e+00, 2.5898e+00,\n",
      "        1.2732e+00, 8.7682e+00, 7.1425e+00, 0.0000e+00, 0.0000e+00, 4.8366e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 5.1419e+00, 0.0000e+00, 0.0000e+00,\n",
      "        2.9248e+00, 5.7290e+00], device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1.])\n",
      "0.3837455213069916\n",
      "tensor([4.9057, 0.0000, 0.1859, 0.0000, 6.4262, 0.0000, 6.6490, 2.3952, 2.8118,\n",
      "        0.0000, 0.0000, 7.8768, 0.1939, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 5.9195, 0.0000, 3.1215, 6.5466, 0.0000, 0.0000, 0.0000, 6.0547,\n",
      "        8.0959, 0.0000, 2.9523, 6.8745, 2.6414], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.])\n",
      "0.39446014165878296\n",
      "tensor([ 1.1220,  0.0000,  0.0000,  0.0000,  0.0000,  3.4602,  0.0000,  0.4743,\n",
      "         6.8319,  0.0000,  0.0000,  1.8216,  0.0000,  0.0000,  0.0000,  8.1583,\n",
      "         0.0000,  9.1180,  0.0000,  3.8370,  6.7730,  0.0000, 10.4945,  0.0000,\n",
      "         0.0000,  0.0000, 13.0444,  6.1001,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.])\n",
      "0.4986809194087982\n",
      "tensor([0.0000, 3.7407, 0.0000, 0.4345, 3.5691, 5.5436, 3.8024, 4.5922, 6.1051,\n",
      "        0.0000, 2.2484, 3.3006, 0.0000, 0.0000, 5.9849, 0.0000, 6.3078, 0.0000,\n",
      "        0.0000, 0.0000, 3.9679, 0.0000, 2.2325, 4.9354, 0.0000, 0.0000, 6.0902,\n",
      "        0.0000, 6.0038, 3.6824, 0.0000, 6.1545], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.])\n",
      "0.3446195125579834\n",
      "tensor([ 7.2908,  0.0000,  0.0000,  2.5093,  0.0000,  0.0000, 10.4474,  7.6355,\n",
      "         0.0000,  8.5798,  1.6222,  0.0000,  0.0000,  0.0000,  0.0000,  6.5921,\n",
      "         0.0000,  8.5871,  0.0000,  0.0000,  0.0000,  0.0000,  5.5739,  0.0000,\n",
      "         0.0000,  0.0000,  2.8502,  7.1620,  0.0000,  0.0000,  5.6581,  4.4445],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1.])\n",
      "0.47278931736946106\n",
      "tensor([0.0000, 0.0000, 2.8170, 3.5587, 4.8694, 3.3194, 0.0000, 4.8378, 7.0446,\n",
      "        7.5918, 3.5742, 3.5756, 0.2631, 0.0000, 4.4667, 0.0000, 0.0000, 0.0000,\n",
      "        2.0832, 0.0000, 2.7558, 0.0000, 5.0180, 1.7304, 5.2777, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 6.1789, 0.0000, 0.0000], device='cuda:0',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<SqueezeBackward1>) tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.])\n",
      "0.36026936769485474\n",
      "tensor([5.3532, 0.0000, 0.0000, 0.0000, 9.6597, 0.0000, 0.0000, 0.7741, 7.2072,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 6.5106, 0.0000, 3.8386, 8.5620, 0.0000,\n",
      "        3.4275, 7.4604, 3.8053, 1.1464, 0.0000, 0.0000, 0.0000, 0.0000, 6.9514,\n",
      "        0.0000, 0.0000, 3.4100, 0.0000, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0.])\n",
      "0.4598608613014221\n",
      "tensor([2.1319, 6.2035, 3.3787, 1.6171, 0.0000, 0.0000, 5.0459, 0.2965, 0.0000,\n",
      "        3.4802, 1.2367, 3.9181, 0.0000, 3.0364, 0.0000, 4.7740, 6.7876, 0.0000,\n",
      "        0.0000, 0.0000, 3.4946, 7.5116, 8.9196, 0.0000, 0.0000, 2.3085, 0.0000,\n",
      "        5.6131, 0.0000, 0.0000, 0.7274, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0.])\n",
      "0.35873574018478394\n",
      "tensor([0.0000, 4.2150, 0.0000, 5.9136, 5.5201, 0.0000, 6.6416, 0.0000, 0.0000,\n",
      "        2.5234, 0.0000, 0.0000, 0.0000, 0.0000, 5.0415, 8.5283, 0.0000, 0.0000,\n",
      "        3.9274, 0.0000, 0.9220, 0.0000, 3.2057, 0.0000, 4.4876, 9.7187, 0.0000,\n",
      "        4.2063, 0.0000, 0.0000, 0.0000, 4.3063], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.])\n",
      "0.4067685604095459\n",
      "tensor([ 0.0000,  0.0723,  8.1702,  0.0000,  0.0000,  0.0000,  9.1458,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  4.7453,  7.2702,  0.0000,  0.0000,  0.0000,\n",
      "         9.6555,  0.0000,  2.6841, 10.7583,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.9293,  0.0000,  0.0000,  0.0000,  0.0000,  6.0854,  8.3898,  5.5061],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.])\n",
      "0.4690026044845581\n",
      "tensor([ 2.8494,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 12.5902,  2.1877,\n",
      "         0.0000,  2.5591,  4.2197,  1.0485,  1.3438,  0.0000,  3.7662,  5.4019,\n",
      "         4.0635,  1.7230,  4.5395,  0.0000,  0.0000,  4.3978,  0.0000,  0.6659,\n",
      "         3.9230,  4.2165,  3.0544,  2.7788,  2.6088,  0.0000,  2.1448,  0.0000],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.])\n",
      "0.3355042338371277\n",
      "tensor([4.7361, 0.0000, 4.3082, 5.0352, 0.0000, 7.6747, 5.4363, 7.4927, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 5.5338, 0.0000, 4.4751, 0.0000, 7.0482,\n",
      "        0.0000, 8.8652, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.7474,\n",
      "        1.8866, 8.3791, 7.3388, 3.4410, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.])\n",
      "0.37595564126968384\n",
      "tensor([ 0.0000,  0.0000, 10.3532,  0.0000,  0.0000,  5.8082,  0.4946,  0.0000,\n",
      "         6.5974,  2.6689,  0.0000,  2.9917,  3.4419,  0.0000,  8.2271,  0.0000,\n",
      "         0.0000,  4.4387,  0.0000,  0.0000,  0.0000, 10.5988,  0.0000,  1.8468,\n",
      "         0.0000,  7.5085,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  3.4162],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.])\n",
      "0.6178897619247437\n",
      "tensor([ 0.0000,  0.0000,  6.2440,  5.5294,  5.7961,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  6.9912,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  8.0231,\n",
      "         4.0980,  7.5967,  0.0000,  6.8819,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         2.9360,  0.0000,  0.0000,  4.1716,  4.3068,  0.0000,  0.4121, 10.7036],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.])\n",
      "0.44370824098587036\n",
      "tensor([ 0.0000,  3.5802,  7.3147,  0.0000,  0.0000,  0.0000,  9.4826,  9.9814,\n",
      "         8.8626,  6.9650,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  5.7415,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 11.7935,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  4.1391,  0.0000,  0.7369,  4.9904,  0.0000,  0.0000],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0.])\n",
      "0.46882346272468567\n",
      "tensor([3.9243, 4.8431, 0.0000, 9.4638, 0.0000, 0.0000, 3.2480, 0.0000, 3.7700,\n",
      "        0.0000, 1.5696, 0.0000, 2.9360, 8.2203, 0.0000, 0.0000, 7.8382, 0.0000,\n",
      "        0.0000, 5.7005, 3.7877, 0.0000, 4.3198, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        1.8885, 6.0623, 0.0000, 5.4024, 5.8268], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1.])\n",
      "0.3628065288066864\n",
      "tensor([0.0000, 0.0000, 0.0000, 2.1433, 5.5821, 1.1964, 0.0000, 0.0000, 0.0000,\n",
      "        5.1519, 6.9930, 5.4730, 0.0000, 0.0000, 0.0000, 0.0000, 0.0446, 5.2899,\n",
      "        2.3793, 0.0000, 7.8903, 0.0000, 0.0000, 8.9679, 0.0000, 3.0645, 0.0000,\n",
      "        0.0000, 0.0000, 7.4408, 6.0305, 6.9639], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1.])\n",
      "0.4058724343776703\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.6147, 1.5182, 0.0000, 0.0000,\n",
      "        7.5463, 5.7935, 0.1736, 0.0000, 0.0000, 3.9084, 4.4927, 5.4353, 9.7661,\n",
      "        6.7510, 1.5118, 0.0000, 0.0000, 0.0000, 0.3586, 0.0000, 6.8594, 6.4791,\n",
      "        0.0000, 0.0000, 7.1182, 0.0000, 2.2941], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.])\n",
      "0.4470534324645996\n",
      "tensor([8.5873, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7433, 0.0000, 4.8190, 0.0000, 2.8056, 6.0204, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.8005, 1.0748, 0.0000, 0.0000, 9.1475, 0.0000, 0.0000,\n",
      "        0.0000, 6.1437, 7.4771, 0.0000, 8.0466], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1.])\n",
      "0.5352956056594849\n",
      "tensor([0.0000, 5.1589, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        3.9626, 0.0000, 0.0000, 6.3866, 0.0000, 0.0000, 6.3177, 6.8511, 7.3308,\n",
      "        5.0229, 0.0000, 6.1477, 0.0000, 4.5352, 0.0000, 0.0000, 0.0000, 3.4779,\n",
      "        0.0000, 9.2142, 0.0000, 4.1974, 7.0112], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.])\n",
      "0.4145393371582031\n",
      "tensor([0.0000, 0.0000, 3.6257, 0.0000, 7.6543, 0.0000, 0.0000, 6.1473, 0.0000,\n",
      "        3.1446, 3.6593, 0.0000, 0.0000, 0.0000, 0.2712, 4.7995, 4.4674, 0.0000,\n",
      "        0.0000, 0.0000, 9.7059, 0.0000, 7.4665, 7.9513, 0.0000, 0.0000, 2.8750,\n",
      "        0.0000, 4.2617, 0.0000, 0.0000, 3.3026], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.])\n",
      "0.42302024364471436\n",
      "tensor([0.0000, 9.8177, 0.0000, 0.0000, 0.0000, 1.5090, 0.0643, 0.0000, 4.6903,\n",
      "        0.3136, 0.0000, 0.0000, 6.8777, 0.0000, 1.2193, 7.3728, 0.0000, 4.7181,\n",
      "        4.6284, 0.0000, 9.2027, 0.0000, 8.5552, 0.0000, 0.0000, 0.0000, 8.1072,\n",
      "        6.0809, 0.0000, 0.0000, 0.0000, 6.8150], device='cuda:0',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<SqueezeBackward1>) tensor([1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1.])\n",
      "0.4713408350944519\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.5646,  0.0000,  5.5455,  8.7503,  8.9479,\n",
      "         0.0000,  0.4511,  0.0000,  0.0000,  0.0000,  0.0000, 13.1739,  7.8363,\n",
      "         0.0000,  0.0000,  2.6981,  0.0000,  0.0000,  0.0000,  6.3981, 10.0460,\n",
      "         0.0000,  0.0000,  4.1890,  0.0000,  0.0000,  0.0000,  7.7575,  0.0000],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
      "        1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0.])\n",
      "0.4971431493759155\n",
      "tensor([6.7782, 3.8308, 0.0000, 2.9512, 0.0000, 0.0000, 0.0000, 0.0000, 4.7506,\n",
      "        0.0000, 4.3639, 0.0000, 0.0000, 9.2395, 0.0000, 9.4509, 0.0000, 6.6414,\n",
      "        0.3115, 0.0000, 0.0000, 5.9664, 6.2528, 6.3587, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 4.1509, 6.4990, 0.0000, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.])\n",
      "0.42053985595703125\n",
      "tensor([6.0523, 6.3814, 0.0000, 0.0000, 0.0000, 4.0943, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 5.3467, 0.0000, 7.3539, 6.7610, 0.6467, 0.0000, 0.0000, 0.0000,\n",
      "        5.1680, 6.5481, 5.4502, 0.0000, 4.9638, 2.5582, 4.0854, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 5.4980, 0.0000, 6.2036], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.])\n",
      "0.3858605623245239\n",
      "tensor([0.0000, 4.7826, 2.3490, 0.0000, 0.0000, 0.0000, 0.0000, 6.0731, 2.8715,\n",
      "        0.0000, 1.5249, 3.5853, 0.0000, 2.5097, 0.0000, 7.8146, 3.5250, 0.0000,\n",
      "        8.2979, 0.0000, 3.3579, 3.0543, 0.0000, 4.1181, 4.7769, 0.3642, 0.0000,\n",
      "        0.0000, 0.0000, 9.8843, 0.0000, 0.5622], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0.])\n",
      "0.40300148725509644\n",
      "tensor([0.0000, 1.5689, 0.0000, 0.0000, 7.3050, 0.0000, 9.9300, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 2.3812, 0.0000, 0.0000, 8.3485, 0.0000,\n",
      "        7.5620, 0.0000, 1.7662, 0.0000, 0.0000, 4.7980, 3.5967, 0.0000, 9.4329,\n",
      "        9.1606, 0.0000, 4.7499, 0.6437, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.])\n",
      "0.4598960280418396\n",
      "tensor([6.2547, 8.3941, 5.2608, 2.2937, 0.0000, 0.0000, 0.0000, 5.2582, 0.0000,\n",
      "        4.8680, 0.0000, 0.0000, 1.7460, 6.1544, 0.0000, 4.0396, 7.5978, 0.0000,\n",
      "        5.7624, 0.0000, 0.0000, 0.0000, 6.0742, 0.0000, 5.8923, 0.0000, 0.0000,\n",
      "        0.0000, 3.7890, 0.0000, 0.0000, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.])\n",
      "0.4001375436782837\n",
      "tensor([0.0000, 0.0000, 8.0135, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        3.7750, 5.8007, 0.0000, 8.5902, 7.8137, 1.9169, 0.0000, 3.3174, 8.3037,\n",
      "        0.0000, 1.5951, 0.0000, 4.5704, 0.3536, 0.0000, 0.0000, 6.6258, 0.0000,\n",
      "        5.9348, 4.0304, 0.0000, 0.0000, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.])\n",
      "0.43057551980018616\n",
      "tensor([0.0000, 5.7620, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 6.5999, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 9.2692, 6.4678, 3.3498, 0.5152,\n",
      "        0.0000, 2.4352, 0.0000, 0.0000, 6.9189, 7.9968, 0.0000, 4.6376, 6.2949,\n",
      "        5.2975, 0.0000, 0.0000, 0.0000, 4.4293], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.])\n",
      "0.44711291790008545\n",
      "tensor([ 0.0000,  0.0000,  6.4658,  5.5534,  4.7116,  1.7530,  6.3707,  5.8844,\n",
      "        10.2382,  2.9247,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  5.9459,  0.0000,  5.3051,  1.7702,  0.0000,\n",
      "         4.9572,  0.0000,  4.2966,  6.1647,  0.0000,  0.0000,  1.8967,  0.0000],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1.])\n",
      "0.38567954301834106\n",
      "tensor([0.0000, 6.0220, 0.0000, 0.0000, 0.0000, 7.2212, 1.0079, 5.7455, 3.6961,\n",
      "        0.0000, 5.3658, 6.2699, 9.9212, 0.0000, 0.0000, 0.0000, 0.0000, 1.5893,\n",
      "        4.4702, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.4185, 0.0000, 8.2597,\n",
      "        9.3841, 0.0000, 0.0000, 0.0000, 6.6264], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1.])\n",
      "0.40800803899765015\n",
      "tensor([ 0.0000,  0.0000,  5.3579,  0.0000,  0.0000,  9.3954,  0.0000,  0.0000,\n",
      "         3.2578, 10.2112,  1.5623,  0.1573,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  3.5489,  0.0000,  8.9648,  0.0000,  0.0000,  5.3862,\n",
      "         4.8964,  0.0000,  0.0000,  0.0000, 13.2696,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.])\n",
      "0.5364587903022766\n",
      "tensor([0.0000, 0.0000, 4.2524, 2.8004, 0.0000, 0.0640, 0.0000, 0.0000, 0.0000,\n",
      "        6.4222, 0.0000, 0.0000, 0.0000, 0.2895, 4.5938, 7.8201, 5.0383, 6.7637,\n",
      "        5.7354, 7.3356, 0.0000, 3.4173, 0.0000, 0.0000, 3.1274, 0.0000, 0.0000,\n",
      "        9.5404, 0.0000, 0.0000, 8.0449, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0.])\n",
      "0.4228052496910095\n",
      "tensor([2.5110, 0.0000, 0.0000, 4.7594, 2.5595, 2.5958, 0.0000, 5.5317, 7.4643,\n",
      "        0.0000, 7.5034, 0.0000, 3.6178, 1.5030, 0.0000, 1.1426, 7.8430, 5.9729,\n",
      "        2.5865, 5.1670, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 4.4849,\n",
      "        0.0000, 0.0000, 4.1182, 4.5480, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0.])\n",
      "0.35183215141296387\n",
      "tensor([8.8852, 0.0000, 9.1205, 7.7065, 0.0000, 5.3833, 4.2369, 0.9428, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 7.0779, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        5.1030, 2.3950, 6.2989, 0.0000, 0.0000, 0.0000, 2.3694, 7.6231, 0.0000,\n",
      "        5.1096, 0.0000, 5.4963, 0.0000, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0.])\n",
      "0.4363762140274048\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  6.4521, 14.3785,  1.2707,  0.0000,\n",
      "         6.7896,  0.0000,  0.0000,  1.0730,  0.0000, 10.7121,  0.0000,  0.0000,\n",
      "         0.0000,  6.1877,  0.8400,  0.0000,  5.2851,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  8.0601,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1.])\n",
      "0.5647728443145752\n",
      "tensor([ 0.0000,  1.4938,  8.8781,  0.0000,  0.0000,  0.0000,  2.2282,  0.6299,\n",
      "         7.0663,  0.0000,  2.6702,  5.9608,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         5.7842,  1.5271,  0.0000,  0.0000,  6.4320,  0.0000, 11.7363,  0.0000,\n",
      "         2.7666,  5.0934,  2.7082,  0.0000,  3.3905,  0.0000,  9.8497,  0.0000],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0.])\n",
      "0.3830859363079071\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  5.6154, 10.5985,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  5.0016,  0.5551,  0.0000,  0.0000,  6.7029,  0.0000,\n",
      "         6.8591,  0.0000,  0.0000,  0.0000,  6.6081,  0.0000,  6.9750, 11.0999,\n",
      "         0.0000,  0.9349,  2.4612,  0.0000,  0.0000,  0.0000,  4.6530,  6.3166],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1.])\n",
      "0.5163717865943909\n",
      "tensor([2.3151, 3.4891, 0.0000, 0.0000, 0.0000, 0.0000, 3.2696, 0.0000, 0.0000,\n",
      "        1.1480, 3.4280, 5.3745, 0.7041, 3.8876, 4.8723, 6.2164, 0.0000, 0.0000,\n",
      "        2.2322, 2.5883, 6.0915, 0.0000, 0.0000, 1.3463, 6.5081, 1.6448, 1.9544,\n",
      "        4.9674, 1.4019, 7.7462, 0.0000, 4.2973], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.])\n",
      "0.2965427041053772\n",
      "tensor([4.5984, 0.0000, 6.4429, 3.5860, 7.0871, 0.0000, 2.4063, 0.0000, 4.8062,\n",
      "        0.0000, 0.0000, 1.7434, 3.8023, 8.8829, 0.0000, 4.0304, 4.2283, 1.4462,\n",
      "        0.0000, 0.0000, 2.9165, 0.0000, 0.0000, 0.0000, 5.8579, 0.0000, 0.0000,\n",
      "        5.4684, 0.0000, 5.5294, 0.0000, 6.0272], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.])\n",
      "0.41971367597579956\n",
      "tensor([7.4749, 5.8529, 0.0000, 7.2158, 0.0000, 0.0000, 0.0000, 6.8611, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 4.8688, 0.0000, 0.0000, 0.0000, 0.0000, 7.6648,\n",
      "        5.2756, 4.2430, 0.0000, 0.0000, 0.9706, 0.0000, 1.7754, 3.8470, 9.1195,\n",
      "        3.0485, 0.0000, 6.1746, 0.2786, 1.2398], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.])\n",
      "0.3989626169204712\n",
      "tensor([ 0.0000,  0.0000,  5.3901,  0.0000,  8.4480,  0.0000,  2.6591,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  7.1331,  5.4471,  0.3618,  6.2911,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.9746,  0.0000,  0.0000,\n",
      "         1.8494,  0.0000,  0.0000,  0.0000,  1.2141,  0.0000, 10.2166,  8.7297],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1.])\n",
      "0.5693203210830688\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 5.7901, 0.0000, 3.1482, 6.5713,\n",
      "        0.0000, 7.2034, 8.1400, 0.0000, 2.3503, 0.0000, 7.9938, 7.0498, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 1.3843, 0.0000, 8.6341, 5.4011, 0.0000, 7.5251,\n",
      "        6.5968, 0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0.])\n",
      "0.4231163263320923\n",
      "tensor([8.0153, 0.0000, 0.0000, 3.9370, 7.7111, 0.0000, 5.9258, 0.0000, 5.0123,\n",
      "        8.2324, 0.0000, 4.7538, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        5.7175, 0.0000, 6.8209, 6.7943, 0.0000, 6.3129, 2.4036, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.3285], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.])\n",
      "0.423026442527771\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  5.8552,  0.0000,  3.9370,\n",
      "         0.0000,  6.5135,  0.0000,  0.0000,  0.0000,  8.1834,  3.4622,  2.0480,\n",
      "         5.6004,  5.4814,  0.0000,  0.0000, 10.1337,  5.5620,  0.0000,  0.0000,\n",
      "         7.0981,  5.2243,  0.0000,  5.3058,  0.0000,  0.0000,  0.0000,  4.6882],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1.])\n",
      "0.3964007794857025\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  5.5781,  8.2479,  6.1167,  0.0000,  3.0561,\n",
      "         0.0000,  0.8210,  1.8678,  8.2770,  0.0000,  8.7683,  0.0000,  3.4383,\n",
      "         1.9458,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  6.2408,\n",
      "         0.9068,  0.0000,  0.0000,  0.0000,  5.2358,  2.2240, 13.0762,  0.0000],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0.])\n",
      "0.43061667680740356\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0713,  0.0000,  3.7925,\n",
      "         0.0000, 11.1266,  3.5567,  7.9784,  0.0000,  0.0000,  2.9586,  8.5013,\n",
      "         0.0000,  7.8258,  0.1135,  0.0000,  3.0240,  0.0000,  0.0000,  0.0000,\n",
      "         0.9244,  0.0000,  0.0000,  7.9395,  0.0000,  0.0000,  6.9917,  0.1398],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.])\n",
      "0.5954883694648743\n",
      "tensor([7.7659e+00, 4.2987e+00, 0.0000e+00, 0.0000e+00, 6.4115e+00, 0.0000e+00,\n",
      "        0.0000e+00, 5.9225e+00, 3.8999e-03, 0.0000e+00, 8.8846e+00, 6.9089e+00,\n",
      "        0.0000e+00, 0.0000e+00, 3.1658e+00, 0.0000e+00, 6.0075e+00, 4.0126e-01,\n",
      "        0.0000e+00, 0.0000e+00, 2.2449e-01, 0.0000e+00, 9.8928e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.9773e+00,\n",
      "        0.0000e+00, 7.2647e+00], device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.])\n",
      "0.4676132798194885\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  4.8490,  0.0000,  0.0000,  9.2484,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.3220,\n",
      "         0.0000,  3.6715,  2.9945,  6.0246,  4.6050,  0.0000,  0.0000,  9.7067,\n",
      "         0.0000,  0.0000,  8.7593, 15.2141,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0.])\n",
      "0.47990262508392334\n",
      "tensor([ 0.0000,  0.0000,  4.0347,  0.0000,  9.1374,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  8.7630,  0.0000,  0.0000,  0.0000,  6.5170,\n",
      "         0.0000,  8.2234,  0.0000,  0.0000, 10.6072,  5.3351,  0.0000,  0.0000,\n",
      "         0.0000,  0.1707,  0.0000,  0.0000,  0.0000,  9.3382,  8.1744,  0.0000],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.])\n",
      "0.4964189827442169\n",
      "tensor([3.2657, 0.0000, 6.5045, 0.0000, 2.9732, 0.0000, 7.7794, 0.0000, 0.0000,\n",
      "        9.6104, 6.7692, 0.0000, 0.0000, 0.0737, 0.0000, 6.2562, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 6.0594, 3.6893, 2.7409, 2.7685, 0.0000, 6.4894, 0.0000,\n",
      "        0.0000, 0.0000, 1.9074, 5.2496, 5.6398], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1.])\n",
      "0.38164111971855164\n",
      "tensor([0.0000, 6.7274, 0.0000, 0.0000, 3.6674, 6.9903, 3.5721, 5.8147, 0.0000,\n",
      "        2.2694, 0.0000, 0.0000, 4.4806, 0.0000, 0.0000, 6.8951, 3.8608, 0.0000,\n",
      "        6.8974, 0.0000, 0.0000, 2.7945, 0.0000, 0.0000, 0.0000, 8.3544, 0.0000,\n",
      "        0.0000, 0.3726, 3.4889, 6.0243, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.])\n",
      "0.393444299697876\n",
      "tensor([0.0000, 3.1565, 5.0930, 0.0000, 0.0000, 0.0000, 0.0000, 2.7170, 5.4842,\n",
      "        0.0000, 1.0244, 1.6333, 0.0000, 0.0000, 0.0000, 6.0431, 4.6967, 0.0000,\n",
      "        5.8051, 4.7866, 0.0000, 2.6004, 3.6910, 4.8278, 4.5041, 7.1059, 0.0000,\n",
      "        2.0657, 1.8095, 4.2395, 3.5938, 0.0000], device='cuda:0',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<SqueezeBackward1>) tensor([0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.])\n",
      "0.3144753575325012\n",
      "tensor([0.0000, 2.8224, 3.9671, 2.6501, 5.1974, 0.0000, 6.5688, 0.0000, 0.0000,\n",
      "        0.0000, 1.9747, 0.0000, 0.0000, 6.2320, 2.9256, 8.7969, 4.4011, 4.0630,\n",
      "        2.9239, 0.0000, 3.7133, 2.4078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.8790, 0.0000, 5.1616, 4.8505, 1.0682], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.])\n",
      "0.3402531147003174\n",
      "tensor([ 0.0000,  0.0000,  3.6451,  0.0000,  0.0000,  0.0000,  6.7622,  7.4946,\n",
      "         0.0000,  8.8281,  0.0000, 11.9047,  2.1142,  0.0000,  0.0000,  7.7349,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  1.7860,  0.0000,  6.3141,  0.0000,\n",
      "         0.0000,  3.3701,  0.0000,  0.0000,  5.7266,  0.0000,  8.6890,  0.0000],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.])\n",
      "0.49953335523605347\n",
      "tensor([0.0000, 0.0000, 0.0000, 4.9786, 0.0000, 0.0000, 0.0000, 4.2040, 0.0000,\n",
      "        0.7340, 2.8581, 0.0000, 6.4149, 5.7605, 5.1727, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 7.7380, 0.0000, 0.0000, 5.2029, 0.0000, 5.6946, 4.1513,\n",
      "        3.1489, 5.9387, 0.0000, 6.7126, 4.4793], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.])\n",
      "0.3857961893081665\n",
      "tensor([ 8.3363,  0.0000,  2.1414,  0.0000,  0.0000,  3.1681,  9.1696,  0.0000,\n",
      "         0.0000,  0.0000,  1.4679,  0.0000,  7.8011,  3.7158,  2.7469,  0.0000,\n",
      "         3.7933,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  5.5907, 10.0867,\n",
      "         0.0000,  2.4723,  0.9131,  0.0000,  5.1543,  0.0000,  5.3338,  0.0000],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1.])\n",
      "0.42494505643844604\n",
      "tensor([1.9652, 2.5022, 0.0000, 5.9037, 0.0000, 0.0000, 0.0000, 2.8123, 0.9675,\n",
      "        3.9452, 3.8030, 0.0000, 0.0000, 1.6070, 0.0000, 5.2902, 5.9422, 3.4126,\n",
      "        3.6677, 3.4879, 0.0000, 3.0564, 6.5869, 1.3885, 0.0000, 8.9993, 0.0000,\n",
      "        1.2137, 0.0000, 0.0000, 0.0000, 6.9105], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1.])\n",
      "0.3267053961753845\n",
      "tensor([ 6.6506,  0.0000,  4.2403,  6.6553,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         4.2800,  7.6319,  0.0000,  0.0000,  3.0177,  4.4861,  0.0000,  9.2859,\n",
      "         0.0000,  7.0101,  0.6350,  0.0000, 10.4023,  4.0907,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  7.7990,  0.0000,  1.0608,  0.0000],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.])\n",
      "0.4356957972049713\n",
      "tensor([0.0000, 3.3706, 5.1184, 6.4225, 0.0000, 0.0000, 0.0000, 1.4280, 1.2893,\n",
      "        2.4907, 0.0000, 4.8868, 5.7399, 5.5223, 7.9535, 5.3624, 0.0000, 3.9451,\n",
      "        0.0000, 0.0000, 0.0000, 4.6538, 2.4626, 0.0000, 4.9666, 0.0000, 0.0000,\n",
      "        6.0660, 0.0000, 1.1679, 0.0000, 2.4174], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1.])\n",
      "0.3368411958217621\n",
      "tensor([0.0000, 0.0000, 0.0000, 2.3736, 4.5352, 1.2767, 0.9155, 3.7582, 6.3487,\n",
      "        0.0000, 6.9862, 0.0000, 0.0000, 1.0838, 3.5276, 0.0000, 2.2090, 3.5522,\n",
      "        0.0000, 4.3333, 6.2604, 5.2227, 0.0000, 5.5964, 6.9481, 0.0000, 2.8095,\n",
      "        3.9009, 0.0000, 0.0000, 6.5003, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.])\n",
      "0.3211534023284912\n",
      "tensor([8.7658, 7.1567, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 5.5248, 0.0000,\n",
      "        1.6136, 0.0000, 0.0000, 0.0000, 3.2563, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        7.5513, 0.0000, 0.0000, 4.7734, 0.0000, 4.4657, 4.4811, 0.0000, 8.8099,\n",
      "        0.0000, 5.8252, 0.0000, 6.0705, 3.9219], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.])\n",
      "0.42033711075782776\n",
      "tensor([2.8813, 3.1662, 0.0000, 0.0000, 3.5514, 0.0000, 0.0000, 3.1824, 2.7532,\n",
      "        2.7436, 2.9815, 4.9519, 0.0000, 0.0531, 4.8467, 0.0000, 6.7104, 0.0000,\n",
      "        2.2330, 0.0000, 0.0000, 7.8989, 0.0000, 0.0000, 0.0000, 0.0000, 2.8142,\n",
      "        0.0000, 5.1566, 7.0302, 6.6547, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0.])\n",
      "0.3620927929878235\n",
      "tensor([0.0000, 5.1849, 0.0000, 0.0000, 6.2497, 0.0000, 7.2446, 4.6601, 6.4103,\n",
      "        8.2458, 0.0000, 0.0000, 0.0729, 0.0000, 0.0000, 0.0000, 6.0915, 1.8746,\n",
      "        0.0000, 0.0000, 2.6979, 0.0000, 0.0000, 0.0000, 0.0000, 6.3597, 6.8569,\n",
      "        3.4937, 1.6792, 3.7385, 6.3435, 4.6706], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "0.3600827157497406\n",
      "tensor([7.6934, 8.4537, 0.0000, 0.0000, 0.0000, 7.5723, 0.0000, 4.2265, 3.5315,\n",
      "        7.0057, 2.8137, 0.4306, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 9.0546,\n",
      "        0.0000, 0.0000, 2.6002, 0.0000, 0.0000, 5.3597, 0.0000, 6.9778, 0.0327,\n",
      "        0.0000, 0.0000, 0.0000, 4.9808, 4.6618], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1.])\n",
      "0.42466169595718384\n",
      "tensor([0.0000, 0.0000, 0.0000, 5.7334, 0.0000, 2.8099, 0.0000, 0.0000, 0.0000,\n",
      "        9.5519, 7.7573, 0.0000, 0.0000, 7.1136, 0.0000, 7.0318, 0.0000, 0.0000,\n",
      "        1.1442, 4.0502, 0.0000, 2.7545, 5.2935, 5.0210, 0.0000, 0.0000, 0.0000,\n",
      "        6.5898, 3.0257, 0.0000, 0.0000, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0.])\n",
      "0.4623020887374878\n",
      "tensor([ 8.3184,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  3.8573,  6.7147,  0.0000,  0.0000,  7.2357,  7.5545,  0.0000,\n",
      "         0.0000,  0.0000,  2.6185,  8.8151,  0.0000,  4.4868,  0.0183,  4.6548,\n",
      "         0.0000, 10.4225,  0.0000,  5.0173,  0.0000,  0.0000,  3.5848,  6.6963],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
      "        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1.])\n",
      "0.4165322482585907\n",
      "tensor([0.0000, 8.0309, 0.0000, 9.9677, 6.8061, 9.5020, 0.0000, 0.0000, 4.5341,\n",
      "        8.8940, 0.0000, 0.0000, 0.0000, 0.0000, 9.1741, 0.0000, 0.0000, 0.0000,\n",
      "        6.0240, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8450, 7.0843, 0.0000,\n",
      "        0.5443, 0.0000, 0.0000, 6.2089, 1.1493], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.])\n",
      "0.5255116820335388\n",
      "tensor([0.0000, 0.0000, 8.9747, 6.1733, 0.0000, 0.0000, 5.3359, 5.7951, 0.0000,\n",
      "        4.8522, 0.0000, 0.0000, 7.2196, 0.0000, 0.0000, 0.0000, 0.0000, 3.9622,\n",
      "        4.4815, 0.0000, 0.0000, 0.0000, 0.0000, 4.3405, 0.0000, 0.0000, 6.5905,\n",
      "        3.1958, 0.0000, 4.2747, 9.5301, 0.0000], device='cuda:0',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<SqueezeBackward1>) tensor([0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0.])\n",
      "0.41521206498146057\n",
      "tensor([4.0522, 0.0000, 0.0000, 2.2498, 2.8745, 0.0000, 4.9168, 0.0000, 5.2306,\n",
      "        7.0365, 0.0000, 0.0000, 0.0000, 5.6177, 7.3123, 1.8557, 0.0000, 0.0000,\n",
      "        7.2953, 3.8030, 0.0000, 0.0000, 1.1540, 0.0000, 2.4619, 0.0000, 8.1052,\n",
      "        6.2865, 8.1326, 0.0000, 0.0000, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.])\n",
      "0.3689664900302887\n",
      "tensor([8.6612, 0.0000, 4.2769, 0.0000, 5.5932, 0.0000, 6.1233, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 7.6719, 0.0000, 0.3004, 6.2172, 3.6806, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 5.7484, 3.3584, 0.0000, 0.0000, 6.9436, 0.0000, 7.2089,\n",
      "        7.2441, 2.3417, 0.0000, 0.0000, 6.3646], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.])\n",
      "0.3911951184272766\n",
      "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0243e+00, 0.0000e+00,\n",
      "        0.0000e+00, 6.2474e+00, 7.6468e-04, 0.0000e+00, 3.1534e+00, 0.0000e+00,\n",
      "        0.0000e+00, 1.2618e+01, 6.9564e+00, 9.5152e+00, 0.0000e+00, 0.0000e+00,\n",
      "        3.6731e+00, 5.9473e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2523e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 5.6094e+00, 7.4060e+00, 0.0000e+00,\n",
      "        0.0000e+00, 3.8423e+00], device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1.])\n",
      "0.4370068907737732\n",
      "tensor([6.6259, 6.2778, 6.1521, 0.3633, 0.0000, 5.1858, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 6.3623, 0.0000, 0.0000, 2.1466, 3.0742, 0.0000,\n",
      "        0.0000, 0.9852, 8.1663, 0.0000, 8.9413, 0.0000, 1.1525, 4.5280, 0.0000,\n",
      "        0.0000, 8.6815, 0.0000, 8.4393, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
      "        0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.])\n",
      "0.4448593854904175\n",
      "tensor([0.0000, 0.0000, 4.3992, 0.0000, 4.8914, 0.0000, 7.7825, 5.5509, 0.0000,\n",
      "        0.0000, 2.6187, 0.0000, 1.3828, 0.0000, 0.0000, 5.3522, 2.7057, 0.0000,\n",
      "        4.7796, 2.9089, 1.4914, 4.5191, 5.8490, 4.9094, 4.2301, 1.1971, 7.3178,\n",
      "        0.0000, 0.0000, 3.9656, 0.0000, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.])\n",
      "0.3335946500301361\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 3.0624, 7.9182, 2.3602, 0.0000, 3.5288,\n",
      "        0.0000, 6.6577, 0.0000, 0.0000, 0.0000, 7.0996, 6.2123, 7.2641, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 8.8321, 0.0000, 0.0000, 5.8823, 0.0000,\n",
      "        7.6223, 0.0000, 0.0000, 4.3112, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.])\n",
      "0.4390532970428467\n",
      "tensor([0.0000, 0.0000, 2.7484, 3.4564, 6.3717, 4.5171, 1.1291, 3.8545, 0.0000,\n",
      "        5.3303, 2.2366, 5.8664, 5.4751, 4.4236, 0.0000, 0.0000, 0.0203, 0.0000,\n",
      "        0.0000, 4.4218, 3.4751, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.0052,\n",
      "        0.0000, 5.8167, 4.3313, 0.0000, 5.2931], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1.])\n",
      "0.3447185754776001\n",
      "tensor([1.7757, 5.7673, 5.0570, 3.7947, 5.2491, 0.0000, 7.2890, 5.2704, 1.4140,\n",
      "        3.7366, 0.0000, 3.8317, 0.0000, 0.0000, 0.0000, 8.0423, 0.0000, 0.0000,\n",
      "        4.6403, 3.2790, 0.0000, 0.6842, 0.0000, 5.5056, 0.0000, 6.7994, 0.0000,\n",
      "        0.0000, 1.7384, 0.0000, 1.9701, 0.0000], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0.])\n",
      "0.3412173390388489\n",
      "tensor([ 1.1795,  5.3650,  0.0000,  4.2434,  1.4126,  0.0000, 10.6300,  0.0000,\n",
      "         0.0000,  0.0000,  3.6560,  0.0000,  0.0000,  0.0000,  0.0000,  8.1613,\n",
      "         0.0000,  8.8557,  6.4414,  5.7920,  6.4661,  0.0000,  0.0000,  4.7868,\n",
      "         3.7659,  1.8950,  0.0000,  4.2001,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.])\n",
      "0.434978723526001\n",
      "tensor([ 4.2873,  6.9333,  0.0000,  0.0000,  0.6890,  0.0000,  5.6552,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  9.1171,  2.4984,  0.0000,  0.0000,  3.6135,\n",
      "         0.0000,  2.2511,  0.0000,  6.1029, 15.6812,  0.0000,  4.7434,  0.0000,\n",
      "         5.7098,  0.0000,  0.0000,  4.7490,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.])\n",
      "0.43198204040527344\n",
      "tensor([0.0000e+00, 2.4657e+00, 1.3500e+00, 3.3106e+00, 8.1752e+00, 1.2839e-01,\n",
      "        5.1750e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 5.0224e-05, 0.0000e+00, 0.0000e+00, 2.2873e+00, 1.6738e-01,\n",
      "        0.0000e+00, 4.8329e+00, 0.0000e+00, 0.0000e+00, 4.7344e+00, 7.2048e+00,\n",
      "        8.6583e+00, 1.1035e+01, 3.1705e+00, 0.0000e+00, 7.5683e+00, 3.6369e+00,\n",
      "        0.0000e+00, 0.0000e+00], device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.])\n",
      "0.4536222815513611\n",
      "tensor([11.2004,  0.0000,  0.0000,  5.3271,  0.0000,  0.0000,  0.0000,  8.7229,\n",
      "         9.1031,  0.0000,  5.2568,  4.5071,  0.0000,  1.4739,  8.1446,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  7.3424,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         7.0108,  0.0000,  0.0000,  0.8465,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0.])\n",
      "0.5192580819129944\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.7784, 0.0000, 0.0000, 0.0000, 7.5526, 0.0000,\n",
      "        7.8672, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        5.0400, 0.0000, 9.7076, 0.0000, 2.6397, 0.0000, 9.2952, 9.1538, 1.7765,\n",
      "        0.0000, 0.0000, 2.8369, 0.0000, 7.1659], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>) tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1.])\n",
      "0.5556085705757141\n",
      "tensor([ 6.1176,  0.0000,  0.0000,  0.0000,  4.4343, 10.1368,  8.2445,  0.0000,\n",
      "         0.0000,  7.2511,  3.7418,  3.6692,  0.0000,  0.0000,  2.8111,  3.7181,\n",
      "         1.3301,  5.5083,  5.9790,  0.0000,  2.1368,  0.0000,  3.9017,  0.0000,\n",
      "         4.6800,  0.0000,  0.0000,  0.0000,  0.0000,  6.5242,  0.0000,  0.0000],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.])\n",
      "0.40468549728393555\n",
      "tensor([9.4427, 0.0000, 0.0000, 0.0000, 5.2393, 4.4866, 0.0000, 0.0000, 4.4416,\n",
      "        0.0000, 0.0000], device='cuda:0', grad_fn=<SqueezeBackward1>) tensor([1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.])\n",
      "0.4436640441417694\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_iterator):\n",
    "            \n",
    "    # unpack batch\n",
    "    x, fasttext_x, seqlen_x, y, fasttext_y, seqlen_y, classes = batch\n",
    "    x, fasttext_x, seqlen_x, y, fasttext_y, seqlen_y = x.cuda(), fasttext_x.cuda(), seqlen_x.cuda(), y.cuda(), fasttext_y.cuda(), seqlen_y.cuda()\n",
    "\n",
    "    # make prediction\n",
    "    predictions = model(x, fasttext_x, seqlen_x, y, fasttext_y, seqlen_y)\n",
    "    print(predictions, classes)\n",
    "    print(criterion(predictions, classes.cuda()).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
