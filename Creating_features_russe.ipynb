{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_path = \"D:/dialogue2020/taxonomy-enrichment/data/training_data/training_nouns.tsv\"\n",
    "verbs_path = \"D:/dialogue2020/taxonomy-enrichment/data/training_data/training_verbs.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring_program.evaluate import *\n",
    "from scoring_program.utils import *\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from operator import itemgetter\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines.vectorizers.fasttext_vectorizer import FasttextVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "from ruwordnet.ruwordnet_reader import RuWordnet\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = read_dataset(nouns_path, lambda x: json.loads(x))\n",
    "verbs = read_dataset(verbs_path, lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "ft = FasttextVectorizer(\"baselines/models/cc.ru.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rwn = RuWordnet(db_path=\"dataset/ruwordnet.db\", ruwordnet_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_path = \"wiki_ru.jsonlines\"\n",
    "wiki_vectors_path = \"baselines/models/vectors/fasttext/ru/wiki.txt\"\n",
    "rwn_vectors_path = \"baselines/models/vectors/fasttext/ru/rwn_full.txt\"\n",
    "node2vec_nouns_path = \"D:\\\\dialogue2020\\\\diachrony_for_taxonomy_enrichment\\\\data\\\\node2vec\\\\node2vec_ru_nouns.txt\"\n",
    "node2vec_verbs_path = \"D:\\\\dialogue2020\\\\diachrony_for_taxonomy_enrichment\\\\data\\\\node2vec\\\\node2vec_ru_verbs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"[^А-я \\-]\")\n",
    "delete_bracets = re.compile(r\"\\(.+?\\)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiktionary(path):\n",
    "        wiktionary = {}\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                wiktionary[data['word']] = {\"hypernyms\": data['hypernyms'], \"synonyms\": data['synonyms'],\n",
    "                                            \"meanings\": data['meanings']}\n",
    "        return wiktionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiktionary = get_wiktionary(wiki_path)\n",
    "wiki_model = KeyedVectors.load_word2vec_format(wiki_vectors_path, binary=False)\n",
    "rwn_model = KeyedVectors.load_word2vec_format(rwn_vectors_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselines.vectorizers.projection_vectorizer import ProjectionVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_nouns = ProjectionVectorizer(ft.model, \"../ru_projection_verbs\")\n",
    "\n",
    "projection_verbs = ProjectionVectorizer(ft.model, \"../ru_projection_verbs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2vec_nouns_model = KeyedVectors.load_word2vec_format(node2vec_nouns_path, binary=False)\n",
    "node2vec_verbs_model = KeyedVectors.load_word2vec_format(node2vec_verbs_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_associates(neologism, topn):\n",
    "    vector = ft.get_multiword_vectors([neologism])[0]\n",
    "    return rwn_model.similar_by_vector(vector, topn)\n",
    "\n",
    "def compute_hchs(neologism, topn):\n",
    "    associates = map(itemgetter(0), generate_associates(neologism, topn))\n",
    "    hchs = [hypernym for associate in associates for hypernym in rwn.get_hypernyms_by_id(associate)]\n",
    "    return hchs\n",
    "\n",
    "def distance2vote(d, a=3.0, b=5.0, y=1.0):\n",
    "    sim = np.maximum(0, 1 - d ** 2 / 2)\n",
    "    return np.exp(-d ** a) * y * sim ** b\n",
    "\n",
    "def compute_distance(s):\n",
    "    return np.sqrt(2*(1-s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_candidates(neologism, topn):\n",
    "    hypernyms = compute_hchs(neologism, topn)\n",
    "    second_order_hypernyms = [s_o for hypernym in hypernyms for s_o in rwn.get_hypernyms_by_id(hypernym)]\n",
    "    all_hypernyms = Counter(hypernyms + second_order_hypernyms)\n",
    "    associates = generate_associates(neologism, 100)\n",
    "    votes = Counter()\n",
    "    for associate, similarity in associates:\n",
    "        distance = compute_distance(similarity)\n",
    "        for hypernym in rwn.get_hypernyms_by_id(associate):\n",
    "            votes[hypernym] += distance2vote(distance)\n",
    "            for second_order in rwn.get_hypernyms_by_id(hypernym):\n",
    "                votes[second_order] += distance2vote(distance, y=0.5)\n",
    "    return all_hypernyms, votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(word, candidate):\n",
    "    v1 = ft.get_multiword_vectors([word])[0]\n",
    "    v2 = rwn_model[candidate]\n",
    "    v1 = v1 / (sum(v1 ** 2) ** 0.5)\n",
    "    v2 = v2 / (sum(v2 ** 2) ** 0.5)\n",
    "    return 1 - spatial.distance.cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_nouns = read_dataset(\"D:/dialogue2020/dialogue2020_shared_task_hypernyms/dataset/private/nouns_private_subgraphs.tsv\", lambda x: json.loads(x))\n",
    "private_verbs = read_dataset(\"D:/dialogue2020/dialogue2020_shared_task_hypernyms/dataset/private/verbs_private_subgraphs.tsv\", lambda x: json.loads(x))\n",
    "public_nouns = read_dataset(\"D:/dialogue2020/dialogue2020_shared_task_hypernyms/dataset/public/nouns_public_subgraphs.tsv\", lambda x: json.loads(x))\n",
    "public_verbs = read_dataset(\"D:/dialogue2020/dialogue2020_shared_task_hypernyms/dataset/public/verbs_public_subgraphs.tsv\", lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: 1525, in wiki: 1501,  in hyper: 292, in synonyms: 16, in def: 536\n",
      "All: 350, in wiki: 350,  in hyper: 11, in synonyms: 10, in def: 99\n",
      "All: 762, in wiki: 741,  in hyper: 143, in synonyms: 12, in def: 251\n",
      "All: 175, in wiki: 173,  in hyper: 4, in synonyms: 4, in def: 46\n"
     ]
    }
   ],
   "source": [
    "def count_statistics(data):\n",
    "    pr_count = 0\n",
    "    in_hypernyms = 0\n",
    "    in_synonyms = 0\n",
    "    in_definition = 0\n",
    "    n_cands = 0\n",
    "\n",
    "    for word, hypernyms in data.items():\n",
    "        hypernyms = [j for i in hypernyms for j in i]\n",
    "\n",
    "        if word.lower() in wiktionary:\n",
    "            pr_count += 1\n",
    "            \n",
    "            hyp = False\n",
    "            syn = False\n",
    "            def_ = False\n",
    "            \n",
    "            wiktionary_data = wiktionary[word.lower()]\n",
    "            \n",
    "            for candidate in hypernyms:\n",
    "                n_cands += 1\n",
    "                candidate_words = delete_bracets.sub(\"\", rwn.get_name_by_id(candidate)).split(',')\n",
    "                \n",
    "                if any([candidate_word.lower() in wiktionary_data['hypernyms'] for candidate_word in candidate_words]):\n",
    "                    hyp = True\n",
    "                    #in_hypernyms += 1\n",
    "\n",
    "                if any([candidate_word.lower() in wiktionary_data['synonyms'] for candidate_word in candidate_words]):\n",
    "                    #in_synonyms += 1\n",
    "                    syn = True\n",
    "\n",
    "                if any([any([candidate_word.lower() in i for candidate_word in candidate_words])\n",
    "                        for i in wiktionary_data['meanings']]):\n",
    "                    #in_definition += 1\n",
    "                    def_ = True\n",
    "            if hyp:\n",
    "                in_hypernyms += 1\n",
    "            if syn:\n",
    "                in_synonyms += 1\n",
    "            if def_:\n",
    "                in_definition += 1\n",
    "                \n",
    "    print(f\"All: {len(data)}, in wiki: {pr_count},  in hyper: {in_hypernyms}, in synonyms: {in_synonyms}, in def: {in_definition}\")\n",
    "\n",
    "count_statistics(private_nouns)\n",
    "count_statistics(private_verbs)\n",
    "count_statistics(public_nouns)\n",
    "count_statistics(public_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights(neologism, candidate, count, hyponym_count, node2vec_similarity):\n",
    "    similarity = get_similarity(neologism, candidate)\n",
    "    wiki_similarity = 0.0\n",
    "    not_wiki_similarity = 0.0\n",
    "    in_synonyms = 0.0\n",
    "    in_hypernyms = 0.0\n",
    "    in_definition = 0.0\n",
    "    not_in_synonyms = 0.0\n",
    "    not_in_hypernyms = 0.0\n",
    "    not_in_definition = 0.0\n",
    "    \n",
    "    if hyponym_count == 0.0:\n",
    "        not_hyponym_count = 1.0\n",
    "    else:\n",
    "        not_hyponym_count = 0.0\n",
    "    \n",
    "    candidate_words = delete_bracets.sub(\"\", rwn.get_name_by_id(candidate)).split(',')\n",
    "    if neologism.lower() in wiktionary:\n",
    "        wiktionary_data = wiktionary[neologism.lower()]\n",
    "        \n",
    "        if any([candidate_word.lower() in wiktionary_data['hypernyms'] for candidate_word in candidate_words]):\n",
    "            in_hypernyms = 1.0\n",
    "        else:\n",
    "            not_in_hypernyms = 1.0\n",
    "            \n",
    "        if any([candidate_word.lower() in wiktionary_data['synonyms'] for candidate_word in candidate_words]):\n",
    "            in_synonyms = 1.0\n",
    "        else:\n",
    "            not_in_synonyms = 1.0\n",
    "            \n",
    "        if any([any([candidate_word.lower() in i for candidate_word in candidate_words])\n",
    "                for i in wiktionary_data['meanings']]):\n",
    "            in_definition = 1.0\n",
    "        else:\n",
    "            not_in_definition = 1.0\n",
    "            \n",
    "        wiki_similarities = []\n",
    "        for wiki_hypernym in wiktionary_data['hypernyms']:\n",
    "            wiki_hypernym = wiki_hypernym.replace(\"|\", \" \").replace('--', '')\n",
    "            wiki_hypernym = pattern.sub(\"\", wiki_hypernym)\n",
    "            if not all([i == \" \" for i in wiki_hypernym]):\n",
    "                wiki_similarities.append(compute_similarity(wiki_hypernym.replace(\" \", \"_\"), candidate))\n",
    "        if wiki_similarities:\n",
    "            wiki_similarity = sum(wiki_similarities)/len(wiki_similarities)\n",
    "        else:\n",
    "            not_wiki_similarity = 1.0\n",
    "    else:\n",
    "        not_wiki_similarity = 1.0\n",
    "            \n",
    "    return np.array([count*similarity, wiki_similarity, not_wiki_similarity, in_synonyms, not_in_synonyms, in_hypernyms, not_in_hypernyms, in_definition, not_in_definition, hyponym_count, not_hyponym_count, node2vec_similarity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node2vec_similarity(model, v1, candidate):\n",
    "    v2 = model[candidate]\n",
    "    v1 = v1 / (sum(v1 ** 2) ** 0.5)\n",
    "    v2 = v2 / (sum(v2 ** 2) ** 0.5)\n",
    "    return 1 - spatial.distance.cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(wiki, candidate):\n",
    "        v1 = wiki_model[wiki]\n",
    "        v2 = rwn_model[candidate]\n",
    "        v1 = v1 / (sum(v1 ** 2) ** 0.5)\n",
    "        v2 = v2 / (sum(v2 ** 2) ** 0.5)\n",
    "        return 1 - spatial.distance.cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 25376/25376 [20:08<00:00, 21.00it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6806/6806 [08:07<00:00,  7.85it/s]\n"
     ]
    }
   ],
   "source": [
    "word_candidate_pair = []\n",
    "feature_vectors = []\n",
    "labels = []\n",
    "\n",
    "for neologism, true_hypernyms in tqdm.tqdm(nouns.items()):\n",
    "    if \" \" not in neologism and len(neologism)>3:\n",
    "        true_hypernyms = [j for i in true_hypernyms for j in i]\n",
    "        counts, votes = compute_candidates(neologism, 10)\n",
    "        candidates = set(counts).union(set(votes))\n",
    "        _, node2vec_vector = projection_nouns.predict_projection_word(neologism, node2vec_nouns_model)\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            count = counts.get(candidate, 1)\n",
    "            \n",
    "            if candidate.endswith('-N'):\n",
    "                node2vec_similarity = get_node2vec_similarity(node2vec_nouns_model, node2vec_vector, candidate)\n",
    "\n",
    "                weights = compute_weights(neologism, candidate, count, votes.get(candidate, 0.0), node2vec_similarity)\n",
    "\n",
    "                word_candidate_pair.append((neologism, candidate))\n",
    "                feature_vectors.append(weights)\n",
    "                labels.append(int(candidate in true_hypernyms))\n",
    "            \n",
    "        for true_h in true_hypernyms:\n",
    "            if true_h not in candidates:\n",
    "                \n",
    "                node2vec_similarity = get_node2vec_similarity(node2vec_nouns_model, node2vec_vector, true_h)\n",
    "                \n",
    "                weights = compute_weights(neologism, true_h, 1, 1, node2vec_similarity)\n",
    "                \n",
    "                word_candidate_pair.append((neologism, true_h))\n",
    "                feature_vectors.append(weights)\n",
    "                labels.append(1)\n",
    "                \n",
    "for neologism, true_hypernyms in tqdm.tqdm(verbs.items()):\n",
    "    if \" \" not in neologism and len(neologism)>3 and neologism.lower() in wiktionary:\n",
    "        true_hypernyms = [j for i in true_hypernyms for j in i]\n",
    "        counts, votes = compute_candidates(neologism, 10)\n",
    "        candidates = set(counts).union(set(votes))\n",
    "        _, node2vec_vector = projection_verbs.predict_projection_word(neologism, node2vec_verbs_model)\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            count = counts.get(candidate, 1)\n",
    "            \n",
    "            if candidate.endswith('-V'):\n",
    "                node2vec_similarity = get_node2vec_similarity(node2vec_verbs_model, node2vec_vector, candidate)\n",
    "\n",
    "                weights = compute_weights(neologism, candidate, count, votes.get(candidate, 0.0), node2vec_similarity)\n",
    "\n",
    "                word_candidate_pair.append((neologism, candidate))\n",
    "                feature_vectors.append(weights)\n",
    "                labels.append(int(candidate in true_hypernyms))\n",
    "            \n",
    "        for true_h in true_hypernyms:\n",
    "            if true_h not in candidates:\n",
    "                node2vec_similarity = get_node2vec_similarity(node2vec_verbs_model, node2vec_vector, true_h)\n",
    "                \n",
    "                weights = compute_weights(neologism, true_h, 1, 1, node2vec_similarity)\n",
    "                \n",
    "                word_candidate_pair.append((neologism, true_h))\n",
    "                feature_vectors.append(weights)\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_vectors_large_new_proper.jsonlines\", 'w', encoding='utf-8', newline='\\n') as w:\n",
    "    for (neologism, candidate), vector, label in zip(word_candidate_pair, feature_vectors, labels):\n",
    "        w.write(json.dumps({\"neologism\": neologism, \"candidate\": candidate, \"vector\": list(vector), \"label\": label})+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 2573751, 1: 59914})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
